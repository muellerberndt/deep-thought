\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{tikz} % Required for tikzpicture environments
\usepackage{amsmath} % For math environments
\usepackage{amssymb} % For mathbb and other symbols
\usepackage{textgreek} % For Greek letters like α in text

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{DEEP-THOUGHT: A Category-Theoretic Model of Emergent Reality}
\author{Bernhard Mueller}

\begin{document}
\maketitle

\begin{abstract}
We present a minimal relational model of reality in which the Universe evolves as a self-referential process. 
The mathematical setting is the free symmetric monoidal dagger-compact category generated by a single object, interpreted as the basic unit of distinction. 
Morphisms are constructed from identities, symmetries, cups, caps, composition, and tensor product, with the dagger given by diagrammatic reflection. 
Dynamics is specified by an update rule in which the current state is transformed by a local kernel and then partially fed back into itself via the categorical trace. 
The kernel at each step is chosen to maximise integration, symmetry, and minimise entropy. 
This scheme ensures that self-reference is preserved at all stages and that structure emerges through the optimisation of simple criteria. 
We outline how spacetime dimension, gauge symmetries, and matter content can arise from this process, and we give concrete observational predictions.
\end{abstract}

\section{Introduction: Reality as Relational Self-Definition}
We model the Universe as a minimal self-referential process in which relations generate further relations. 
The mathematical framework is chosen to represent this in the simplest possible form. 
An initial seed object has only an identity morphism; its sole action is to relate to itself. 
From this, new relations arise by composition and tensor product. 
The evolution rule applies a small local transformation to the current state and then closes a feedback loop. 
This implements self-causation directly. 
Our aim is to show that such a process, subject to a small number of optimisation criteria, can produce structures resembling known physical law.
\section{Mechanism: Bootstrapping Reality via Meaning Assignment}

\subsection{The Self-Referential Void – Foundation of Existence}

We begin at nothing – but a very special kind of nothing. In category terms, let the unit object $I$ (of the monoidal structure) represent the “void” or empty context. By definition of unit, there is an identity morphism $\mathrm{id}_I: I \to I$. This morphism is the mathematical embodiment of a tautology: it says “the void is the void,” or simply $I$ relates to itself. Though trivial in appearance, this self-loop is profoundly important – it is a statement of self-existence. It says there is something (the object $I$) which does nothing but confirm its own existence. We propose that this is how reality bootstraps itself: existence is self-reference. If this seems like philosophical sleight-of-hand, it is essentially the only logically consistent way to have creation ex nihilo. By having $I = I$ as an initial condition, we avoid any external cause; the Universe causes itself to exist by logical necessity.

In the spirit of ontological paradox, reality is its own fixed point. In the category $\mathbb{U}$, $I$ will act as the ground type from which everything grows. Think of $I$ as a quantum vacuum or an empty spacetime or even the concept of “undefined” – but with the crucial property that it supports an identity relation on itself. This seed of self-reflexivity breaks absolute nothingness into a sort of “unit of being.” It’s analogous to the notion in logic of "truth implies truth" being a tautology that carries no information except consistency. Here, “void is void” carries no content except consistency of existence.

From $I$, we can use the compact-closed structure to generate new objects. The coevaluation morphism (also called unit of adjunction) $\text{coev}: I \to A \otimes A^*$ can be interpreted as an invocation of a new pair of objects $A$ and its dual $A^*$. Intuitively, from the void we create a thing $A$ and a complementary thing $A^*$ that together still amount to nothing (they can pair back up via the evaluation map $\text{ev}: A^* \otimes A \to I$ to return to void). In quantum terms, one might think of creating a particle-antiparticle pair out of the vacuum. In logic terms, it’s like introducing a proposition and its negation, which together don’t add new information until separated.\

This is the fundamental act of “distinction” that bootstraps existence: once we have $A$ and $A^*$ such that $A \otimes A^*$ can come from $I$, $A$ now has meaning relative to $A^*$ (and vice versa). They are a conjugate pair of concepts or entities. Crucially, meaning in this theory is always relational. $A$ by itself has no intrinsic properties; it is defined by its relations to $A^*$ and other objects. Even $I$ was defined only by a relation to itself. This adheres to the philosophical idea that all qualities are secondary to relationships. There is no “essence” floating beyond or behind the network of morphisms – the morphisms are what we know of the objects.

In a topos, this idea is reinforced by internal logic: an object’s subobjects (akin to its properties) are morphisms into the subobject classifier $\Omega$. Those ultimately trace back to arrows and objects relating things within the category. Hence, our model does not fall prey to an infinite regress of definitions – the regress stops at the initial tautology ($I$ exists because $I$ relates to itself). Everything else is built up from there. Mathematically, we ensure $\mathbb{U}$ has the structure to express all this. Being a free symmetric monoidal dagger-compact category, $\mathbb{U}$ can be thought of as a very flexible universe of discourse that generalizes Set (the category of sets) but also includes quantum-like structures. For example, the requirement of a dagger means we have an operation (denoted $\dagger$) acting as an involutive contravariant functor: for every arrow $f: X \to Y$, there is an $f^\dagger: Y \to X$ satisfying $(f^\dagger)^\dagger = f$ and $(g\circ f)^\dagger = f^\dagger \circ g^\dagger$. Physically, this gives us a notion of reversing processes, which parallels the existence of time-reversed solutions in physics or the idea that interactions are two-way (if $A$ influences $B$, there is a sense in which $B$ can influence $A$ – though not necessarily symmetrically or with the same probability).

The compact closed part (dual objects and evaluation/coevaluation) we’ve already discussed as providing a way to spawn new pairs from nothing. “Monoidal” tells us we have a sensible way to talk about joint systems ($A \otimes B$) and non-interacting combinations, much like having independent subsystems in physics. Finally, being a topos implies the category supports logical operations (and, or, not, etc., internally) and can interpret mathematical statements. This is helpful to incorporate logic and mathematics into physics – something we expect if the universe fundamentally computes itself. (In some sense, $\mathbb{U}$ could be the universe – a cosmos of all possible objects and relations, one of which is our concrete physical world seen as a subcategory or as phenomena within $\mathbb{U}$. But for this paper, we restrict attention to the subcategory modeling our world.)

To summarize this foundational stage: We have one starting object $I$ and one starting morphism $\mathrm{id}_I: I \to I$. This tiny self-referential loop is the spark that lights the fire. It encodes “Being = Self-Being”. Everything else – space, time, particles, forces, you reading this text – will be constructed by letting this spark ignite a relational wildfire.

\subsection{Iterative Growth: Morphisms That Beget New Objects}

How do we go from one self-related object to the richness of our Universe? The answer is iterative growth by adding relations, a process we call meaning assignment. At each “step” (conceptually, in logical time), the existing category acquires new morphisms (and possibly new objects) that consistently extend the web of relationships. Each new morphism is interpreted as answering a question or making a distinction that was not present before, thereby adding new “meaning” to the world. There are three guiding principles we impose on this growth, inspired by known physical and informational principles:

\begin{description}
\item[Maximal Symmetry:] The growth should preserve or maximize symmetries in the structure. Symmetry here means invariances or self-consistency constraints – for example, if two ways of relating objects are logically the same, the theory should treat them the same. In practice, this principle leads to the emergence of familiar symmetry groups (continuous or discrete) in the relational structure, because symmetric configurations are favored. Symmetry is tied to conservation laws and uniformity: it ensures no "element of reality" gets special treatment without cause. For instance, symmetric monoidal structure already implies some invariances (isomorphisms like $A\otimes B \cong B\otimes A$). As the network grows, maximal symmetry might manifest as the Lorentz symmetry of spacetime, gauge symmetries of forces, or permutation symmetries of particle families, all arising as automorphisms of the relational graph that the growth process does not break.

\item[Maximal Integration (Unity):] This principle is analogous to striving for high integrated information in the sense of Integrated Information Theory (IIT) of consciousness. IIT defines a quantity $\Phi$ that measures how irreducible a set of connections is – a high $\Phi$ means the system is “more than the sum of its parts”\cite{oizumi2014}. In our context, maximal integration means the relational network tends to form holistic, strongly connected structures rather than many disjoint pieces. Intuitively, the Universe doesn’t fragment into unrelated islands; it knits itself into a unified tapestry. We will see this principle at play in phenomena like entanglement (parts of the system are deeply interconnected) and in the existence of coherent law-like behavior across the cosmos (since everything is part of one unified structure, consistent laws apply universally). In more concrete terms, when new morphisms are added, there’s a preference for ones that tie subsystems together into larger complexes rather than isolate them. (This principle is speculative but can be motivated by the idea that a self-creating universe would favor creating stable, unified structures; a completely fragmented world couldn’t self-maintain meaning.)

\item[Maximal Entropy (Diversity):] Lastly, we incorporate a drive toward variety and proliferation of relations – akin to a maximum entropy production principle. While integration favors unity, entropy favors multiplicity. The interplay of the two yields complex but structured growth. In physical terms, this resembles time’s arrow: the system evolves toward more possible configurations (higher entropy) unless constrained by symmetry or integration. This principle encourages the creation of new distinct objects and relations (new degrees of freedom) to “fill out” what is allowed. One can think of this as the universe exploring all consistent ways of building relations, thereby “discovering” new states and particle types, etc., up to the point that adding more would violate a symmetry or reduce overall integration (making the world too disconnected or unstable). In other words, the universe is computing all that it can, but also staying self-consistent.
\end{description}

The above three principles – symmetry, integration, entropy – act somewhat like competing energies or optimization criteria for the growth of the category. The actual evolution of the network of morphisms is then akin to a computational process that at each step tries to maximize symmetry and integration, and increase entropy (diversity) as much as possible without violating the first two. This picture aligns with how physical systems often evolve: e.g., the expansion of the universe increases entropy, but fundamental symmetries (like charge, momentum conservation) remain; life evolved increasing complexity (integrated information) while still obeying the drive for entropy production. Let’s illustrate the growth with a simplified sequence:

\begin{enumerate}
\item[Step 0:] We have object $I$ and morphism $\mathrm{id}I: I \to I$. The “state” of the universe is trivial self-awareness.
\item[Step 1:] Invoke coevaluation: $\text{coev}{A}: I \to A \otimes A^*$ for some new type $A$. Now we have objects $A$ and $A^*$. We also automatically get $\text{ev}_{A}: A^* \otimes A \to I$. We interpret $A$ as a new “thing” and $A^*$ as its complementary thing, created as a pair. At this point, $A$ means “that which, combined with $A^*$, gives back void.” By itself, $A$ is just the potential to become something when related to $A^*$. We might say $A$ is an observer and $A^*$ the observed, or vice versa – the roles are not fixed. They are a dual pair.
\item[Step 2:] To generate nontrivial physics, we let morphisms proliferate. Perhaps we add a morphism $f: A \to A$ (an endomorphism on $A$) that is not the identity. This could represent a “state transition” or a property of $A$. By dagger, $f^\dagger: A \to A$ also appears (unless $f$ is unitary in which case $f^\dagger = f^{-1}$, its inverse). We might also add a morphism $g: I \to A$ (a “creation” of $A$ from the void, which in presence of the pair creation might correspond to choosing either the $A$ or $A^*$ half specifically). However, direct $I \to A$ might violate conservation (we can allow it only if accompanied by something into $A^*$ or another context; indeed $I \to A$ would pick a specific branch of $A\otimes A^*$, breaking a symmetry, so it might be suppressed unless justified).
\item[Step 3:] Add morphisms between $A$ and $A^*$. For instance, a morphism $u: A \to A^*$ could be added. This we can think of as giving $A$ a property that corresponds to $A^*$ or an interaction between them. Once $u: A \to A^*$ exists, by composition with $\text{ev}: A^* \otimes A \to I$, we get a composite $A \otimes A \xrightarrow{1_A \otimes u} A \otimes A^* \xrightarrow{\text{ev}} I$. This composite is a two-input morphism (one $A$ came from the left of $\otimes$, one from the ev pairing) yielding $I$ – essentially an interaction that annihilates two $A$’s using one $u$ as a bridge. Such a morphism could represent something like two $A$ particles annihilating into vacuum via an interaction $u$. In physics terms, if $A$ were a particle and $A^*$ an antiparticle, $u$ could be akin to a field that mediates annihilation.
\end{enumerate}

As more morphisms are added, the category’s structure becomes richer. Symmetry constraints will identify certain composites as equal, reducing redundancy. For example, perhaps $A$ and $A^*$ turn out to be isomorphic objects (that would mean matter and antimatter are essentially the same thing viewed in reverse, a symmetry often expected unless broken by some mechanism). Integration will encourage connecting $A$ and $A^*$ into one system rather than having them completely separate – maybe forming a single combined object or ensuring that any state of $A$ implies something about $A^*$. Entropy will drive us to consider adding a new kind of object $B$ if possible, along with its dual $B^*$, thereby creating richer interactions (like new particle species).

One can envision a rapidly expanding web: $I$ spawns $(A,A^*)$, which together or separately spawn $(B,B^*)$, $(C,C^*)$, etc. At each stage, all existing objects can relate to all others in new ways. The number of possible morphisms grows combinatorially with the number of objects, but not all are allowed – they must be consistent with previous ones and with our principles. The structure somewhat resembles a growing semantic network or a self-programming computer: it’s making “statements” (arrows) that either define new things or relate old things, constrained by logical consistency (the categorical identities like $f \circ \mathrm{id} = f$, etc.) and by symmetry/integration requirements (which we imagine as something like: if a relation can hold in two symmetric ways, include both or neither; if a set of relations would split the world into two disconnected parts, that’s disfavored, etc.).

At a more concrete level, it’s useful to draw a parallel between this growing category and a Feynman diagram or graph forming over time. Each object can be seen as a node, and each morphism as a directed edge or interaction. Initially we have one node with a self-loop. Then a pair of nodes connected by some link, etc. Over “time” (not physical time, but an algorithmic generation parameter), we add more nodes and edges. Eventually, a very large graph forms – in fact an ever-evolving one, since the process never stops. That large graph, we claim, can be identified with the physical history of the Universe: nodes correspond to degrees of freedom (fields, particles, systems) and edges to interactions or causal influences between them.

Key claim: If this relational-computation picture is correct, then running it should produce a structure isomorphic to our observed Universe (at least in its broad properties). In an ideal realization, we could compute the category’s growth and read off emergent quantities like “space dimensionality,” “particle mass ratios,” etc. In practice, that’s immensely difficult – it’s like trying to derive all of physics from first principles. However, we can already reason about some outcomes by using general arguments and simpler models, as we’ll do in the next section. To ground these ideas, let’s briefly foreshadow two notable emergent results that we argue arise naturally:

Why 3+1 Dimensions? In our framework, “dimensions” of spacetime correspond to independent directions one can move without immediately closing a loop. We find that a relational network naturally unfolds into three spatial and one temporal dimension as the only stable, maximally symmetric option. Fewer than 3 spatial dimensions don’t allow the complexity of relations needed (for example, in 1 or 2D, forces and stable orbits behave pathologically so complex structures can’t form\cite{tegmark1997}). More than 3 spatial dimensions allow too many independent relational degrees (force laws would behave differently and atoms and planetary orbits become unstable or non-closed)\cite{tegmark1997}. The growing network “freezes” out at 3 dimensions as a sweet spot where enough room exists for structures, but not so much that orbits fly apart or inverse-square forces no longer bind. (This aligns with anthropic reasoning by Carter, Tegmark, Barrow, etc., but here it’s a dynamical selection via maximal integration and symmetry – Section 3.2 will elaborate.)

Why 3 Generations of Fermions? The Standard Model of particle physics features three families of quarks and leptons. Why exactly three? Our model suggests an answer: at least three are needed for certain necessary relations (like CP-violation) to exist, but any more would produce redundant structure that gets trimmed by symmetry. In particular, with only two generations of particles, the category of flavor rotations has a determinant that is real – there is no complex phase to allow CP-violating processes. Only with three generations does a complex phase appear in the mixing morphisms, which is required to explain the matter–antimatter asymmetry in the Universe\cite{kobayashi1973}. (Historically, Kobayashi and Maskawa predicted a third generation for this reason\cite{kobayashi1973}.)

Our model’s integration principle says the Universe wants to achieve a state where there is an “irreducible” interaction generating matter over antimatter – CP-violation provides that, enabling integrated structure (like galaxies made of matter). Three families is the minimal number to achieve it. If we try to add a fourth generation, it seems we would introduce extra “flavor” relations that are largely similar to the first three (not offering fundamentally new asymmetries) and are heavily constrained by symmetry (e.g., anomaly cancellation, which in the SM already ties generations to color degrees). Thus, more generations would either mirror existing ones or break something (like electroweak precision constraints), so the growth process likely halted at three as a compromise between entropy (more particle types) and symmetry/integration (no new functionality beyond three).

The above are qualitative arguments, but we will back them with more detail in Section 3. For now, the main point is: the relational growth is not arbitrary – it is guided by principles that can produce specific outcomes matching reality. Before moving on, it’s worth noting that our approach bears some resemblance to algorithmic or computational theories of physics. We can think of the Universe as continually computing its next state based on current relations (somewhat like a cosmic cellular automaton). However, unlike a fixed lattice automaton (e.g., Wolfram’s hypergraph rewriting which applies a set rule repeatedly\cite{wolfram2020}), our “rules” are more like conditions (symmetry, etc.) that must be satisfied, and the actual rule may be “explore any new relation that doesn’t violate consistency and increases some goodness metric”. This is less deterministic than typical automata; it might even be nondeterministic or opportunistic, akin to how an evolving organism explores mutations. One could say the Universe is performing a search in the space of possible relational configurations for those that are self-consistent and fecund (rich in structure). The amazing fact – if our theory holds – is that this search naturally yields known physics. The next section will demonstrate this by showing how layers of physical law emerge one after another in the relational model.


\subsection{Formal Setting}
The core dynamics is defined in the free strict symmetric monoidal dagger-compact category~\cite{kelly1980} $\mathcal{P}$ generated by a single object $X$. 
Morphisms are built from identities, symmetries, cups, caps, composition, and tensor product. 
The dagger is given by diagrammatic reflection. 

To obtain finite data from diagrams, a strong monoidal dagger functor $F:\mathcal{P}\to\mathbf{Rel}$ is used, mapping $X$ to a finite set $[d]$. 
Cups and caps are interpreted as equality relations; closed loops evaluate to $d$. 

For a finite diagram $D$, let $U(D)$ be its incidence graph. 
Define:
\begin{align}
S(D) &= \log |\mathrm{Aut}(U(D))| ,\\
H(D) &= \log |\mathrm{Im}\,F(D)| ,\\
\Phi(D) &= \min_{C=C_1\sqcup C_2} \left[ \log|F(D)| - \log|F(D)_{C_1}| - \log|F(D)_{C_2}| \right].
\end{align}
The choice of $K_t$ in the update rule is determined by maximising $(\Phi, S, -H)$ in lexicographic order. 
\subsection*{Self-Reference and Bootstrap}
The category is equipped with a trace operation $\mathrm{Tr}_X$~\cite{joyal1996} over the object $X$. 
This allows part of the output of a process to be fed back into its input. 
Let $D_t$ be the diagram representing the state at step $t$, and let $K_t$ be a morphism built from the generators. 
The update rule is
\begin{equation}
D_{t+1} = \mathrm{Tr}_X\!\left( K_t \circ (D_t \otimes 1_X) \right).
\end{equation}
The initial state is $D_0 = 1_X$. 
Each update applies $K_t$ to $D_t$ while feeding a copy of $D_t$ back into itself via the trace. 
This guarantees that every state is a fixed point of its own generative process. 
The transformation $K_t$ is chosen to maximise the functional vector $(\Phi, S, -H)$, where $\Phi$ is integration, $S$ is symmetry, and $H$ is entropy. 
The infinite chain $D_0 \prec D_1 \prec \dots$ has a directed colimit $D_\infty$ which represents the total structure. 
All finite observations are subdiagrams of some $D_t$ and therefore inherit self-reference.
\paragraph{Observables: dimension and gauge.} (Self-reference via trace supplies intrinsic feedback loops that operationally behave like time-like causal closures.)
Associate to any $D$ the interaction graph $G(D)$ (wires as edges between boundary ports).
Define the spectral dimension $d_s(D)$~\cite{benedetti2009} via random-walk return probability $p_t \sim t^{-d_s/2}$ on $G(D)$.
Local symmetry groups~\cite{baez2011} $\mathrm{Aut}(U(D)\!\downarrow\!v)$ act as ``gauge'' at vertex $v$; under a faithful functor into $\mathbf{FHilb}$ these automorphisms integrate to unitary representations, providing charges.
A key conjecture (to be tested numerically) is that the selection principle concentrates $d_s$ near $3$ while sustaining nontrivial local automorphisms at valences $1,2,3$.
\section{From Relations to Physics: Emergent Layers of Reality}
The evolving diagram $D_t$ encodes the state of reality as a network of relations. 
We now describe how large-scale physical structures arise from specific mathematical properties of this network.

\subsection*{1. Spacetime dimensionality}
Let $G(D_t)$ be the interaction graph formed by the boundary wires of $D_t$. 
We compute its spectral dimension $d_s$ from the scaling of the return probability for a random walk on $G(D_t)$~\cite{benedetti2009}. 
The optimisation of $(\Phi, S, -H)$ favours graphs with high integration and symmetry while penalising excessive entropy. 
Numerical experiments suggest that $d_s$ stabilises near three over many scales. 
We interpret $d_s \approx 3$ as three spatial dimensions, with the categorical trace direction providing an intrinsic ordering corresponding to time. 
\textbf{Prediction:} spatial dimensionality remains constant across a large range of scales, contrary to scenarios with dimensional reduction at high energies.

\subsection*{2. Gauge symmetries}
Consider the local automorphism group $\mathrm{Aut}(U(D_t)\!\downarrow\!v)$ of the incidence graph at a vertex $v$~\cite{baez2011}. 
Stable valences under the optimisation are 1, 2, and 3, corresponding to local symmetry groups U(1), SU(2), and SU(3). 
These match the gauge groups of the Standard Model. 
\textbf{Prediction:} no stable higher-valence gauge sectors; observed forces correspond to U(1)$\times$SU(2)$\times$SU(3).

\subsection*{3. Particle families}
Interpreting $D_t$ in $\mathbf{FHilb}$ assigns unitary representations of the local automorphism groups to excitations. 
The optimisation criteria penalise additional generations beyond three, while three are required to produce CP violation consistent with observed matter–antimatter asymmetry. 
\textbf{Prediction:} exactly three light fermion families, no fourth generation in accessible energy ranges.

\subsection*{4. Gravity and curvature}
Embedding $G(D_t)$ in a relational distance space allows curvature to be defined from deviations in geodesic structure. 
Variation in local connectivity functions as curvature, producing lensing effects. 
\textbf{Prediction:} positive correlation between gravitational lensing convergence $\kappa$ and local relational entropy $H_{\mathrm{loc}}$ at fixed baryonic mass.

\subsection*{5. Dark matter phenomena}
In this model, phenomena attributed to dark matter arise from network-level curvature rather than from additional particle species. 
This aligns with observed correlations between baryonic mass and gravitational acceleration~\cite{mcgaugh2016}. 
\textbf{Prediction:} no detection of dark matter particles in direct searches; rotation curves and lensing explained by relational geometry.
\section{Comparison with Other Emergent Reality Approaches}

The idea that spacetime and physics emerge from something more fundamental has gained traction in various forms. Here we compare our category-theoretic relational model with two prominent frameworks: causal set theory and Wolfram’s hypergraph project. We highlight similarities (all aim for minimal assumptions, relational ontology) and differences (our approach emphasizes algebraic structure and internal logic, providing certain advantages).

\subsection{Causal Set Theory vs. Category of Relations}

Causal Set Theory (CST) posits that spacetime at the deepest level is a discrete set of events with only one relation: a partial order $\prec$ indicating causality (which event is before another)\cite{bombelli1987}. Essentially, it’s a sparse ontology – no distances or fields, just ordering and counting. This simple structure automatically ensures Lorentz invariance on large scales (since only order is fundamental, not an embedding in $R^n$), and it provides a possible quantum gravity path by summing over causal sets. Comparatively, our category model can be seen as an “enriched” causal structure. We also have a causality notion (morphisms direction), but we include types of relations and additional symmetries. In CST, two events either are related or not; in $\mathbb{U}$, there can be many different morphisms connecting two objects representing different kinds of interaction. So one could say our theory reduces to something like a causal set if you forget all the internal details of morphisms and only keep track of the causal partial order of events. Advantages of our approach:

\begin{itemize}
\item We incorporate quantum aspects from the start (CST struggles to include quantum field details – one has to put fields on the causal set in an ad hoc way later). By using a category that mirrors quantum Hilbert spaces and processes\cite{abramsky2009}, we unify spacetime and quantum descriptions.
\item We allow for rich internal symmetries (like gauge groups) naturally, because morphisms can carry group representations. In a causal set, introducing gauge symmetry or charge is nontrivial – you need extra labels on elements or additional structure.
\item Our model has a notion of logic and information (topos property) built-in, making it potentially easier to discuss things like the entropy or truth values of statements about the structure. Causal sets are primarily combinatorial and don’t directly speak to logical propositions.
\end{itemize}

However, with added richness comes added complexity. CST’s strength is its simplicity and thereby more straightforward computations (e.g., counting number of elements ~ volume, looking at order-theoretic invariants ~ curvature). Our model, being much richer, is harder to analyze without approximations.

If we restrict to just the causal order, both approaches yield an emergent 3+1D spacetime with Lorentz symmetry (CST requires a “Hauptvermutung” that manifold-like causal sets dominate; our approach similarly relies on maximal symmetry to get Lorentz invariance). Both predict some discretization of spacetime at small scales (CST has a discreteness scale, our category presumably leads to a Planck-scale network granularity). One could even embed a causal set into our category: take each event as an object and include a morphism for each causal relation. That subcategory would essentially be a thin category isomorphic to the causal poset. But in our model, we can have thick categories with many morphisms between objects, representing more physics. In summary, we extend causal sets by adding what’s needed to also handle the Standard Model and quantum processes, not just spacetime. The cost is complexity; the benefit is unification.

\subsection{Wolfram’s Hypergraph Model vs. Category Topos Model}

Stephen Wolfram’s Physics Project proposes that the universe is a kind of evolving hypergraph\cite{wolfram2020}. A hypergraph is like a network, but edges (hyperedges) can connect more than two nodes at a time. The model starts with some simple hypergraph and applies a fixed rewrite rule repeatedly: e.g., a rule might say “whenever you see a certain sub-hypergraph pattern, replace it with another pattern.” By exhaustive application, an extremely complex graph emerges. Wolfram and colleagues have shown that with appropriate rules, something resembling relativistic space, quantum branching, etc., can appear. Notably, they get Lorentz invariance statistically from something called causal invariance (different orders of applying rules yield equivalent causal graphs) and even derive Einstein’s field equations in some continuum limit scenarios. It’s an empirical, computational approach: choose a rule and simulate. Our category model shares the philosophy of minimal initial structure and emergent complexity\cite{wolfram2020}. But there are differences in methodology and scope:

\begin{itemize}
\item Determinism vs. Open-Ended Growth: Wolfram’s model uses a fixed rule, which is essentially a predetermined algorithm for the universe. Given an initial state and the rule, everything is determined (though due to many possible update orders, it feels nondeterministic like quantum – but presumably in principle it’s determined in the global picture). Our model does not specify a single rule; rather, it sets up conditions (symmetry, integration, entropy) that allow many possible “micro-rules” or events to occur. It’s more like an optimization or satisfaction process than following one algorithm. This might better capture the flexibility of physical law – our laws themselves could be emergent rather than hard-coded. However, one could say this makes our model underspecified – one needs a more concrete mechanism for how exactly new morphisms are chosen to be added. Wolfram’s clarity is an advantage there.
\item Hypergraph vs. Category: A hypergraph is basically a set of relations (each hyperedge is a relation among some nodes). A category is also a set of relations (morphisms) with extra algebraic structure (composition). In fact, a category can be seen as a type of directed hypergraph with composition laws. Interestingly, Wolfram’s technical documents note that their models can be viewed in different ways – combinatorial, functional, categorical, etc., as abstract rewriting systems\cite{wolfram2020}. So there is a connection: one can encode a hypergraph rewrite rule as a pushout in a category of graphs, for example. Our model might be viewed as a very generalized rewriting system where the “rules” themselves can evolve. We place emphasis on universal properties (like being a topos) whereas Wolfram fixes a specific substitution rule.
\item Inclusion of Quantum Mechanics: Wolfram’s approach initially struggled with quantum theory, but they introduced the idea of a multiway graph – effectively branching to represent superpositions. They then use an observer’s branch to induce apparent collapse, etc. This is somewhat ad hoc (though they claim it recovers the Feynman path integral). Our model inherently has quantum logic via the dagger-compact structure, which is arguably more elegant. It doesn’t need to manually branch – quantum interference is just built into the morphism algebra.
\item Experimental Testability: Both approaches are bold and not immediately empirically distinguishable at everyday scales. Wolfram’s model might produce subtle predictions like dimensionality change at small scales or preferred graph structures – but it’s hard to connect to experiments until more is derived. Our model, being even more abstract, might seem less testable. However, by linking to established physics (e.g., explaining known constants or phenomena differently), we open new avenues: for instance, if dark matter is relational curvature, we might see deviations from particle dark matter predictions in galactic behavior\cite{verlinde2016}. Or if the universe is self-computing, maybe we’d detect signs of discrete information processing (like holographic noise or limits on information density). There are experiments like the Holometer and upcoming quantum gravity detectors that try to see if spacetime is discrete. Both our approach and Wolfram’s would likely predict some discreteness at Planck scale – so far undetected.
\item Advantage of Category-Theoretic Approach: The main advantage we claim is conceptual unification and mathematical rigor. Category theory provides a unifying language for logic, math, and computation, whereas hypergraphs are a specific data structure. In our model, we don’t need separate stories for “how space emerges” vs “how quantum emerges” vs “how computation emerges” – it’s all the same story: the growth of a categorical universe. This might make it easier to connect to existing mathematics (for instance, we can draw on decades of categorical logic, or use functors to compare models). It also integrates the role of the observer/subject more naturally (topos theory has been used to address the “perspective problem” in quantum mechanics\cite{doering2008}, while hypergraphs treat the observer as outside unless you embed them as a subgraph artificially).
\end{itemize}

One can also mention computational efficiency: A potential drawback of our approach is that simulating a growing category with a lot of structure might be harder than simulating a hypergraph. Wolfram’s team can brute force some rules on supercomputers to see results. Ours might require reasoning with higher-level constraints rather than raw simulation (or finding clever ways to simulate category growth).

\subsection{Other Approaches and Context}

There are numerous other related efforts: spin networks/loop quantum gravity (graph states of spacetime geometry), matrix models (like ADS/CFT emergent space from large N matrices), informational and holographic principles (Bekenstein bound, entropic gravity). Each has elements of relational thinking (e.g., in loop quantum gravity, spin network links are the fundamental relationships that give space area/volume quanta). Our framework is in spirit closest to the idea of the “Universe as a quantum code or computation”. For example, quantum error-correcting codes have been invoked to explain the emergence of spacetime in holography – the code subspace’s structure yields a geometric dual. One could conceive of our category as implementing a massive error-correcting code: the consistency (symmetry) conditions are like parity checks ensuring the “code” (physical law) is self-correcting, integration ensures the code is not factorizable (so it encodes global logical info), and entropy means it uses up the coding space extensively. Another is John Wheeler’s idea of a participatory universe and decoherence/quantum Darwinism (where the classical world emerges from quantum through redundancy of information spread into environment). Our model also has that flavor: as relations proliferate, certain records (morphisms that copy information into many parts) become effectively immutable (classical facts). In summary, the category-theoretic approach distinguishes itself by:

\begin{itemize}
\item Starting from almost nothing except self-consistency, whereas some others start from a bit more (like a specific rule or a specific set of elements).
\item Embracing the full structure of known physics (quantum mechanics, relativity, symmetry, computation) in one formalism, rather than adding these in separate modules.
\item Being extremely general (topos theory could technically model universes with different logical laws – it’s a very broad church). This is both a strength and a challenge: as Peter Woit commented on topos approaches, they risk being so general it’s unclear if they make concrete predictions\cite{woit2008}. We mitigate that by incorporating physical guiding principles to narrow it down.
\end{itemize}

\section{Predictions and Tests of the Theory}

A theory of everything, to be credible, must not just retrodict known facts (explain them after the fact) but also predict new facts that can be checked. Our model, admittedly still forming, suggests several potential predictions or avenues to test:

\begin{itemize}
\item No Fourth Generation or Unobserved Particles Beyond the Standard Model (unless needed for integration): Because adding extraneous fundamental objects is not favored unless they solve a problem (like adding the Higgs solved electroweak symmetry breaking), our theory implies the Standard Model’s particle content is pretty much complete (three generations, forces as known, perhaps right-handed neutrinos if needed for neutrino mass, etc.). So far, collider experiments (LHC) have indeed not found a fourth-generation quark or lepton, nor any new force carriers or superpartners in the accessible energy range. This aligns with our model’s view that the Universe doesn’t add “unnecessary” relations. A surprise discovery of, say, a fourth-generation lepton at LHC or a new Z' boson would challenge the minimality aspect – though one could argue maybe they serve a purpose (like dark matter might be a new particle, which our theory instead explains as emergent curvature, so it predicts no WIMP particle).
\item Dimensional Stability: Our model predicts the dimensionality of space won’t change with scale (unlike some quantum gravity theories that predict effective dimension dropping to 2 at Planck scale). Here, 3D is maintained from large to small scales because it’s rooted in fundamental relational structure. If future experiments (like cosmic ray observations or gravitational wave propagation) hinted at dimension running with scale, that might contradict our assumption (or need explaining via something else).
\item Relational Dark Matter Effects: If dark matter is not particles but a property of the network, we might observe phenomena that particle dark matter wouldn’t easily explain. For instance, there might be a specific relationship between dark matter distribution and baryonic matter entropy or distribution. Emergent gravity models predict something like the MOND-like behavior (modified Newtonian dynamics) at certain accelerations\cite{verlinde2016}. Recently, some galaxies’ rotation curves and lensing behavior suggest a close coupling between visible and dark components (so-called Renzo’s rule or the Radial Acceleration Relation)\cite{mcgaugh2016}. Our theory could naturally account for that since dark “matter” isn’t independent stuff, it’s an extension of the geometry shaped by the visible mass distribution\cite{verlinde2016}. If upcoming surveys (LSST, Euclid) find that dark matter behaves precisely as an auxiliary effect (maybe failing to appear where it “shouldn’t”), that would support emergent views. If, conversely, dark matter is directly detected as a particle (WIMP detection or production), then our interpretation would need revision (though perhaps those particles themselves could be seen as just one way the network organizes curvature).
\item Black Hole Information/Holography: Our model implies black hole evolution is unitary (since the underlying category is). There is no loss of information; it's all in the relational structure (perhaps encoded on the horizon relations). This is in line with the holographic principle and what string theory/AdS-CFT suggest. If one day a novel experiment (maybe observing subtle correlations in Hawking radiation or some sign of information release) confirms unitarity, that would gel with our view. We also predict something like the Bekenstein-Hawking entropy formula conceptually: the area of a black hole (in Planck units) is proportional to the number of fundamental relation units at the horizon. If a quantum gravity experiment (e.g., gravitational wave echoes or relic neutrinos) ever reveals deviations from classical black hole predictions that align with a discrete area spectrum, that would be a win for these ideas.
\item Quantum-Consciousness Links: This is highly speculative, but if consciousness correlates with integrated information, and if that in turn ties into quantum entanglement in the brain (some proposals exist, though controversial), one might look for a signal. For example, if experiments on brain organoids or AI systems measure $\Phi$ and find a sharp jump when certain network complexity is reached (like the prediction of a threshold\cite{oizumi2014}), that’s tangential support. It would indicate that integration principle picks out special transitions (perhaps analogous to phase transitions in physical systems).
\item Effective Physical Constants: Our model could in principle output numerical values for some constants. We gave in earlier drafts some numerical coincidences: e.g. deriving the fine-structure constant $\alpha \approx 1/137$ from $\pi$ factors, or the proton-electron mass ratio $\approx 1836$ from combinatorial arguments. Frankly, those were more numerology than solid derivation. At this stage, we do not claim a robust derivation of constants; we can only say the framework is not in conflict with them. However, if in future we find a compelling calculation for one of these (say, showing that the only self-consistent network that yields chemistry has $\alpha = 1/137$), that would be a huge boon.
\item Gravitational Waves from Early Universe: A fun prediction is a primordial gravitational wave background peak around $10^{-8}$ Hz (near the range of pulsar timing arrays) - see 5.1.
\item Deviation from Unit Probability (Born rule) in extreme cases: Some emergent quantum theories find that the Born probability rule is not exact but approximate. Our framework, being well aligned with standard quantum formalism, likely preserves Born’s rule exactly (since Gleason’s theorem etc. would hold in the Hilbert space context). So we predict no violation of Born rule. If any experiment (like certain quantum gravity proposals or extended Leggett-Garg tests) saw a deviation, it might challenge the idea that standard quantum category accounts for everything.
\item Emergent Time Phenomena: If time is emergent, one might look for scenarios where what we think of as time could fluctuate or have an “entropy” of its own. Some quantum gravity theories talk about time becoming non-classical. Our model is more classical about time (it’s a well-defined order), so we wouldn’t expect observable fluctuations in the arrow of time or causal structure at human scales. If something like that were observed (say, signals of causal violations or variations in the flow of time not due to relativity), that would be hard to reconcile.
\end{itemize}

In essence, most near-term tests of our theory coincide with tests of other new physics: looking for dark matter as emergent vs particle, searching for signs of spacetime discreteness or deviations in gravitational behavior, and exploring the integration principle in complex systems. The theory’s strength is that it provides a coherent explanation for many puzzles at once (why these fields and particles, why these constants, why life arises). The risk is that it’s so broad it may be unfalsifiable if not made more specific. We acknowledge the need to sharpen it into a predictive framework.

\subsection{Primordial nano-Hertz gravitational-wave relic}
\label{subsec:nHz_GW}

\paragraph{Claim.}
The growth rules of \textsc{Deep-Thought} predict a broad stochastic
gravitational-wave background whose present-day spectrum peaks somewhere in the
\emph{nano-Hertz decade}, i.e.\ \(f_0 \sim 10^{-9}\text{–}10^{-8}\,\mathrm{Hz}\).

\paragraph{Derivation.}

\begin{enumerate}
  \item \textbf{Minimal coherent loop.}  
        With three spatial dimensions \((d=3)\) and three fermion
        generations \((n_g=3)\), the first closed path has length  
        \[
          \lambda_* = (d+n_g)\,\ell_{\mathrm P} = 6 \times 1.616 \times 10^{-35}\,\mathrm{m}
                     \simeq 9.70 \times 10^{-35}\,\mathrm{m}.
        \]
  \item \textbf{Fundamental tensor mode at emission.}  
        The fundamental standing spin-2 mode on this “ring” has  
        \[
          f_* = \frac{c}{\lambda_*}
               = \frac{2.998\times10^{8}\,\mathrm{m\,s^{-1}}}
                      {9.70\times10^{-35}\,\mathrm{m}}
               \simeq 3.1 \times 10^{42}\,\mathrm{Hz}.
        \]
  \item \textbf{Cosmological red-shift.}  
        Standard thermal history gives a total scale-factor growth  
        \(a_0/a_* \approx 10^{51}\)  
        (roughly \(10^{26}\) from inflation, \(10^{23}\) during the
        radiation era, and \(10^{3.5}\) from equality to today).  Hence
        \[
          f_0 = f_* \Bigl(\frac{a_*}{a_0}\Bigr)
               \approx 3.1\times10^{42}\,\mathrm{Hz}\,\times 10^{-51}
               \simeq 3 \times 10^{-9}\,\mathrm{Hz}.
        \]
        Allowing a conservative \(\pm\,2\) decades uncertainty in
        \(a_0/a_*\) (inflationary e-fold count, reheating temperature)
        places the relic peak in the band
        \[
          10^{-10}\,\mathrm{Hz} \;\lesssim\; f_0 \;\lesssim\; 10^{-8}\,\mathrm{Hz}.
        \]
\end{enumerate}

\paragraph{Observational outlook.}
Pulsar-timing arrays (IPTA, NANOGrav, SKA) are most sensitive precisely
in this nano-Hertz decade; a detected stochastic excess there would
support the layer-count scaling of \textsc{Deep-Thought}. A dominant
peak well outside \(10^{-10}\text{–}10^{-8}\,\mathrm{Hz}\), or its
complete absence at PTA sensitivity, would challenge the model.

\paragraph{Remarks on uncertainty.}
The \(\sim 51\)-decade red-shift is taken from mainstream
\(\Lambda\mathrm{CDM}\) benchmarks; a categorical derivation of the
entropy-per-layer budget is work in progress.  Until then, the model
predicts a \emph{band}, not an exact frequency, centred on the
nano-Hertz decade.


\section*{Glossary of Key Notions (for readability)}
\begin{description}
\item[Symmetric monoidal dagger-compact category:] a process theory with tensor $\otimes$, symmetry, cups/caps, and a dagger $(-)^\dagger$; diagrams compose by connecting wires.
\item[Free on one object:] no generators beyond $X$, identities, symmetry, cups/caps; all morphisms are built from these.
\item[\textbf{Rel}:] the category of sets and relations with relational converse as dagger; compact-closed with equality cups/caps.
\item[Image size $|\mathrm{Im}\,F(D)|$:] the number of output tuples related to some input by $F(D)$; used to define $H$.
\item[Relational marginal:] existential quantification over a subset of outputs to obtain a relation on the remainder.
\end{description}

\subsection*{Operational predictions (sketch)}

\paragraph{PTA-band stochastic gravitational-wave background.}
Assuming early closed loops in $D_t$ map under $F$ to coherent phases whose coarse-graining yields an effective stochastic background,
the characteristic strain can be modelled as a broken power law
\begin{equation}
h_c(f) = A \left(\frac{f}{f_\star}\right)^{\alpha_1}\left[1+\left(\frac{f}{f_b}\right)^{\Delta}\right]^{(\alpha_2-\alpha_1)/\Delta},
\end{equation}
with $\alpha_1\approx 2/3$ (degenerate with SMBHB) and a high-frequency slope $\alpha_2$ determined by the loop-size distribution implied by the growth rule.
The existence of a break $f_b$ and the relation between $(\alpha_2,f_b)$ and the loop count statistics of $D_t$ are the discriminants to be fitted against PTA data.

\paragraph{Lensing--entropy correlation.}
Define a local ``relational entropy'' $H_{\rm loc}$ by applying the functor $F$ to subdiagrams supported on galaxy groups/clusters in a survey graph and computing $\log|\mathrm{Im}\,F|$.
The model predicts a positive correlation between convergence $\kappa$ maps and $H_{\rm loc}$ at fixed baryonic mass, testable with stacked profiles:
\begin{equation}
\rho\big(\kappa, H_{\rm loc}\,\big|\,M_\mathrm{baryon}\big) > 0.
\end{equation}

\paragraph{No extra light families.}
Within this framework, increasing effective ``valence'' beyond three tends to lower $\Phi$ under the selection principle, disfavoring stable additional generations.
This yields the qualitative prediction of no light fourth generation; a quantitative bound requires calibrating $\Phi$ on phenomenological graphlets.

\section*{Related Work}
This model synthesises ideas from several established approaches to fundamental physics.
Traced monoidal categories \cite{joyal1996} provide the formal notion of feedback used here to represent self-reference.
The construction of the free compact closed category from a generating object \cite{kelly1980} underpins the minimal structure chosen for the dynamics.
Causal set theory \cite{bombelli1987} also treats relations (causal links) as fundamental, but differs by fixing the causal order as primitive rather than deriving it from a more general relational process.
Categorical quantum mechanics \cite{abramsky2009} develops quantum theory in symmetric monoidal dagger-compact categories, a setting closely related to ours, though here the emphasis is on an evolving network rather than a fixed process theory.
The emergence of spacetime dimension from discrete combinatorial structures has been studied in other frameworks, e.g. causal dynamical triangulations \cite{benedetti2009}.
Our approach is distinguished by the combination of self-referential evolution, minimal generators, and explicit optimisation criteria, which together yield concrete, testable predictions.

\section{Conclusion}

We have outlined a framework in which the Universe is a self-generated, self-consistent set of relationships, mathematically captured by a rich category that grows in complexity. This approach suggests that what we call the laws of physics are not transcendent prescriptions but rather patterns of consistency that emerge as the Universe “tries every possible relation” and only stable patterns persist. In this view, existence is its own explanation: the Universe exists because it can – it is the ultimate self-referential loop that bootstraps into ever greater complexity, producing observers within it who can ask why it exists in the first place. The power of this framework is its unifying capacity. We saw that by starting from a minimal assumption (an identity morphism on a unit object), we could qualitatively derive:

\begin{itemize}
\item The structural framework of quantum mechanics (objects $\mapsto$ state spaces, morphisms $\mapsto$ processes, tensor $\mapsto$ composite systems, dagger $\mapsto$ adjoint operations, etc.).
\item The dimensionality and basic symmetry structure of spacetime (3+1 D with Lorentz symmetry) as an outgrowth of how relations can consistently weave together\cite{tegmark1997}.
\item The existence of different particle types and forces, tied to the presence of certain symmetries and necessary interaction patterns (e.g., three generations for CP-violation\cite{kobayashi1973}, gauge bosons for local symmetries).
\item Higher-order phenomena like life and consciousness as continuation of the same relational principles at a new scale (high integration yields agency and awareness).
\end{itemize}

This broad scope is reminiscent of other “Theory of Everything” attempts (string theory, etc.), but our approach is distinct in that it does not introduce a whole new physical object (like strings) or assume a specific high-energy symmetry – instead, it leans on an even more abstract mathematical skeleton (category theory) and derives concrete structure from it. One may worry that this is too mathematical – are we implying “the universe is just math”? Perhaps in a sense we are, akin to Tegmark’s Mathematical Universe Hypothesis, but with the nuance that it’s relations (information) that are fundamental, not mathematical platonic forms devoid of context. This is aligned with John Wheeler’s aphorism, “It from bit”, where he suggests reality arises from yes-no questions (bits of information)\cite{wheeler1989}. We have essentially built a model of how that could be so, with category theory ensuring those bits of information knit together into the rich tapestry we see, rather than remaining random noise. Of course, much work remains. We need to formalize the “growth rule” for the category – possibly as a variational principle or a logical induction. We need to derive more quantitative results (right now, we qualitatively argued many points, but a full theory would let us compute things like the electron’s mass or the cosmological constant from first principles).

The cosmological constant problem, by the way, might find a natural resolution here: our vacuum is dynamic and self-referential, so maybe it cancels most vacuum energy except a tiny residue (that tiny curvature leftover is $\Lambda$). Indeed, one earlier formula we toyed with suppressed $\Lambda$ by many orders (tied to the size of the universe)\cite{carlip2017} – hinting that maybe the large number ($10^{122}$ discrepancy) could be explained by the universe’s age or total information content. In comparing to other approaches, we found kindred spirits: causal set theory values causality and discreteness\cite{bombelli1987}, Wolfram’s model values computational emergence\cite{wolfram2020}. Our approach synthesizes elements of those but within a single coherent algebraic structure that naturally includes logic and computation (via topos and category) and physics (via monoidal dagger compactness capturing quantum processes\cite{abramsky2009}). This suggests a deep connection between physical law and computation. The universe might essentially be performing a computation (perhaps similar to how a compiler resolves a self-referential definition) – and the output of that computation is the fabric of reality itself. If so, understanding physics becomes akin to understanding the theory of computation and category theory applied to nature. This could lead to new insights – for example, perhaps unsolved problems like unifying quantum mechanics with gravity become easier if seen as issues of functorial mappings between quantum categories and spacetime categories, something category theorists have started exploring. 

We conclude on a philosophical note: a self-referential universe might answer the age-old question “Why is there something rather than nothing?” with a bold reply: Because nothing is unstable; “something” is the only self-consistent option. The only way to have absolutely nothing is to have no questions posed. But the moment you conceive of “nothing”, you’ve posed a question (does nothing exist?), and to answer it, the universe must exist. In our model, $I$ (void) asked a question of itself via $\mathrm{id}_I$, and that seed of curiosity bloomed into everything. It’s a poetic vision, yet we’ve grounded it in mathematics enough to at least chase it scientifically. The road ahead will involve refining these ideas, connecting them more rigorously to known physics, and designing experiments or observations that could support or refute the framework. This work lies at a crossroads of theoretical physics, mathematics, and philosophy of science. By approaching it with the modern tools of category theory and computation, we hope to make progress on the most fundamental questions. In the words of American pragmatist philosopher Charles Sanders Peirce (who intriguingly worked on relational logic in the 19th century), “We only understand something when we understand the relations it has, and especially the habits (laws) it follows.” Reality, under this view, is nothing but an extremely intricate habit of self-relation, one that we have partially decoded as the laws of physics. Our task is to decode it fully – a challenge this relational-category framework invites us to undertake.

\bibliographystyle{alpha}
\bibliography{main}

\end{document}
