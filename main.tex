\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{tikz} % Required for tikzpicture environments
\usepackage{amsmath} % For math environments
\usepackage{amssymb} % For mathbb and other symbols
\usepackage{textgreek} % For Greek letters like α in text

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{DEEP-THOUGHT: A Category-Theoretic Model of Emergent Reality}
\author{Bernhard Mueller}

\begin{document}
\maketitle

\begin{abstract}
We propose a maximally simple, relational theory of reality grounded in category theory. In this model, existence arises as a self-referential bootstrap: starting from an initial void'' object that refers only to itself, a network of entities and relations creates subjective meaning without external input. Using a dagger-compact closed monoidal topos as the mathematical framework, the theory naturally incorporates key principles of physics---relativity of observations, causal structure, and quantum mechanics---as emergent properties of the relational network. We show how space, time, and matter can emerge from ``iterative meaning assignment'' morphisms, and how known features of our Universe (e.g., 3+1 dimensions, three generations of fundamental fermions) naturally result from optimizing symmetry, integration, and entropy in the relational graph. We contrast this approach with other emergent reality frameworks (causal set theory, Wolfram's hypergraph model, etc.), highlighting the advantages of a category-theoretic formulation in unifying physics and information. Finally, we
 discuss testable predictions---for example, viewing dark matter as a manifestation of relational curvature rather than a new particle---and suggest ways this model could be experimentally or observationally falsified. This work positions ``reality as relationships'' not just as a philosophical stance but as a rigorous computational theory of everything, from quantum physics to consciousness.

\end{abstract}

\section{Introduction: Reality as Relational Self-Definition}

At the most fundamental level, what exists, and why? Traditional physics takes for granted an arena of space, time, and fields, then seeks laws governing them. Here we pursue a more radical starting point: nothing but relationships. In our approach, entities and their interactions are defined only through each other, such that reality emerges as a self-consistent web of relations. Existence, in this view, is relational meaning: something ``is'' because it plays a role in the mutual definition of a coherent structure. This addresses the classic question ``How come existence?'' in a novel way. Rather than invoking an external creator or an infinite regress of causes, we posit that reality bootstraps itself via self-reference. The Universe essentially says: ``I am that I am'' -- much like a dictionary where words are defined in terms of each other, with no word having meaning in isolation. By being internally consistent, the relational system brings itself into being. As physicist John Wheeler hinted in his it-from-bit metaphor, every particle, every field, even spacetime itself, derives its function, meaning, and very existence entirely ``from yes-or-no queries and their answers''\cite{wheeler1989}. In other words, physical reality may arise from an underlying informational or relational structure -- a participatory universe where asking questions (making observations) helps bring about the answers (the concrete reality)\cite{wheeler1989}.

We use category theory as the language to make these ideas precise. Category theory has been called the ``mathematics of relations and composition''\cite{awodey2010}, focusing not on objects in isolation but on connections (morphisms) between them. It provides an ideal sandbox for defining a Universe where relations are primary. In particular, we work in a dagger-compact closed monoidal topos (denoted $\mathbb{U}$). Though a mouthful, this structure encodes exactly the ingredients needed to derive physics from first principles.

\begin{description}
\item[Objects as Contexts:] Objects in the category represent entities or systems (which could be as simple as a quantum state or as complex as an observer). In a topos, objects also carry an internal logic (like sets or logical propositions), allowing us to treat physical states and logical truths in one framework.

\item[Morphisms as Relations/Processes:] Morphisms (arrows) represent relationships or processes from one object to another. Composition of morphisms encodes how processes chain together (much like cause and effect). Directed morphisms naturally capture causality, since an arrow $A \to B$ can be read as ``$A$ influences $B$'' or ``$A$ comes before $B$'' in a process sense. (This resonates with the causal set approach to quantum gravity, where the fundamental structure is a partial order of events -- a set of ``before/after'' relations that stand in for spacetime\cite{bombelli1987}.)

\item[Dagger ($\dagger$) as Reversibility:] The dagger structure gives each morphism $f: A \to B$ an adjoint $f^\dagger: B \to A$, analogous to taking the transpose or complex conjugate in linear algebra. This encodes reversible processes and relationships. In physical terms, it relates to the idea that processes can in principle be undone or traced backwards (like unitary evolution in quantum mechanics).

\item[Monoidal Tensor ($\otimes$) for Composition of Systems:] The monoidal structure allows us to combine systems into composites ($A \otimes B$), providing a way to build up multipartite systems from parts. The unit object $I$ of the monoidal structure plays the role of an ``empty context'' or vacuum.

\item[Compact Closure (Duals):] Every object $A$ has a dual $A^*$, and there are special morphisms $\text{coev}: I \to A \otimes A^*$ and $\text{ev}: A^* \otimes A \to I$ satisfying certain identities. These encode the idea of creation and annihilation of entity pairs, or coupling an object with a corresponding ``dual'' object to return to the void. This is deeply related to quantum entanglement and feedback loops in processes.

\item[Topos (Internal Logic and Geometry):] Working in a topos means we have a rich internal logical structure (akin to sets, or sheaves on a space). It gives a built-in way to talk about sub-objects (subspaces, propositions) and to handle continuum or geometric aspects if needed. Chris Isham and others have advocated topos theory as a foundation for physics where classical logic is replaced by a more general logical structure to naturally accommodate quantum theory\cite{doering2008}.
\end{description}

Notably, dagger-compact categories axiomatize many structural aspects of quantum mechanics\cite{abramsky2009} (finite-dimensional quantum theory can be seen as a dagger-compact category of Hilbert spaces and linear maps). This means our framework is not put in by hand to imitate physics -- rather, it is general enough that quantum behavior, classical behavior, and even spacetime structures can emerge from the relational axioms. Category theory’s emphasis on relationships perfectly matches core principles of modern physics.

\begin{description}
\item[Relativity of Meaning:] Just as Einstein’s relativity taught us that measurements of space and time are observer-dependent, in our model the properties of any object are defined only relative to other objects. There is no God’s-eye view of an absolute state. This is in line with the relational interpretation of quantum mechanics, which asserts that the state of a system is only defined relative to an observer (or another system) interacting with it\cite{rovelli1996}. Our category’s functors can map between different contexts (subcategories or frames of reference), ensuring that descriptions in different frames are related by consistent transformations rather than an absolute truth. All meaning is contextual -- an echo of the relational quantum idea that quantum mechanics is a theory about the ``physical description of systems relative to other systems''\cite{rovelli1996}.

\item[Causality and Time:] The directed nature of morphisms gives an arrow of time at the fundamental level -- a morphism $f: A \to B$ can be viewed as $A$ preceding or causing $B$. Composition $g \circ f: A \to C$ means $A$ causes $B$, which in turn causes $C$, establishing a causal chain. While the dagger provides formal time-reversal symmetry (for each process one can imagine an adjoint), in our model we will see that a kind of global entropy or growth principle makes the forward direction of composing new morphisms far more likely, yielding an emergent arrow of time. Causality is built-in, rather than imposed. (Compare to causal set theory, which starts from a set of events with only a partial order ``precedes'' and nothing else\cite{bombelli1987}. Our approach is richer: the arrows carry not just order but also content -- they can represent, for example, an interaction or information flow, not just an ordering.)

\item[Observation as Interaction:] In a relational reality, an ``observer'' is just another system interacting and correlating with a system of interest. A measurement outcome is simply the establishment of a relation (morphism) between the observer and the observed. This fits neatly with both quantum foundations and everyday experience -- to observe is to become entangled with. The relational interpretation of quantum theory explicitly says a measurement is an ordinary physical interaction that correlates two systems\cite{rovelli1996}. In our category model, when an observer system $O$ ``measures'' a system $S$, we interpret it as creating a morphism $S \to O$ (or $I \to S \otimes O$) that encodes the acquired information. No collapse needs to be added ad hoc; the act of establishing that relation is the update of state for $O$ relative to $S$. In essence, observation creates new relationships, which is exactly how our Universe grows in this model.
\end{description}

In summary, the hypothesis of this paper is that the Universe can be understood as the output of a computation of relational self-consistency. Starting from an initial trivial state (a ``void'' that only knows itself), a process of iterative meaning assignment generates new structures by answering the question ``what things exist and what do they mean to each other?'' The only source of existence is tautology: the initial object exists because it refers to itself. All other objects then come into existence as meaningful distinctions or relationships relative to what already exists, in a never-ending expansion. This worldview draws inspiration from other approaches that treat information or computation as fundamental, such as Wheeler’s it-from-bit idea\cite{wheeler1989} and digital physics, but it provides a specific, structured realization in terms of category theory. Unlike the Wolfram hypergraph Physics Project which brute-forces physics by evolving simple rewrite rules on graphs\cite{wolfram2020}, or causal set theory which posits a fixed set of events, our approach seeks a principled derivation of known physics from a minimal self-generating premise. It also connects to proposals in quantum gravity that spacetime and gravity are emergent from more fundamental relations (e.g. emergent gravity from entanglement\cite{vanraamsdonk2010}).

The rest of this paper is organized as follows:
\begin{itemize}
\item In Section 2, we describe the mechanism of relational growth in our category-theoretic model, including the role of the initial self-referential object and the guiding principles (symmetry, integration, entropy) that steer the growth.
\item In Section 3, we discuss how familiar layers of physics emerge from this mechanism -- quantum fields, spacetime, particles, and even consciousness as an endpoint of complex information integration. We highlight a few quantitative or qualitative outcomes of the theory, such as why spacetime is 3+1-dimensional and why there are three generations of fermions.
\item Section 4 compares our approach to existing emergent-reality frameworks like causal sets and Wolfram’s hypergraph model, noting differences in simplicity and unifying power.
\item In Section 5 we outline experimental and observational implications of the theory that could provide validation or falsification, including the interpretation of dark matter and the nature of black hole information.
\item Finally, Section 6 concludes with the philosophical and scientific significance of viewing reality as a self-referential network of meaning.
\end{itemize}


\section{Mechanism: Bootstrapping Reality via Meaning Assignment}

\subsection{The Self-Referential Void – Foundation of Existence}

We begin at nothing – but a very special kind of nothing. In category terms, let the unit object $I$ (of the monoidal structure) represent the “void” or empty context. By definition of unit, there is an identity morphism $\mathrm{id}_I: I \to I$. This morphism is the mathematical embodiment of a tautology: it says “the void is the void,” or simply $I$ relates to itself. Though trivial in appearance, this self-loop is profoundly important – it is a statement of self-existence. It says there is something (the object $I$) which does nothing but confirm its own existence. We propose that this is how reality bootstraps itself: existence is self-reference. If this seems like philosophical sleight-of-hand, it is essentially the only logically consistent way to have creation ex nihilo. By having $I = I$ as an initial condition, we avoid any external cause; the Universe causes itself to exist by logical necessity.

In the spirit of ontological paradox, reality is its own fixed point. In the category $\mathbb{U}$, $I$ will act as the ground type from which everything grows. Think of $I$ as a quantum vacuum or an empty spacetime or even the concept of “undefined” – but with the crucial property that it supports an identity relation on itself. This seed of self-reflexivity breaks absolute nothingness into a sort of “unit of being.” It’s analogous to the notion in logic of "truth implies truth" being a tautology that carries no information except consistency. Here, “void is void” carries no content except consistency of existence.

From $I$, we can use the compact-closed structure to generate new objects. The coevaluation morphism (also called unit of adjunction) $\text{coev}: I \to A \otimes A^*$ can be interpreted as an invocation of a new pair of objects $A$ and its dual $A^*$. Intuitively, from the void we create a thing $A$ and a complementary thing $A^*$ that together still amount to nothing (they can pair back up via the evaluation map $\text{ev}: A^* \otimes A \to I$ to return to void). In quantum terms, one might think of creating a particle-antiparticle pair out of the vacuum. In logic terms, it’s like introducing a proposition and its negation, which together don’t add new information until separated.\

This is the fundamental act of “distinction” that bootstraps existence: once we have $A$ and $A^*$ such that $A \otimes A^*$ can come from $I$, $A$ now has meaning relative to $A^*$ (and vice versa). They are a conjugate pair of concepts or entities. Crucially, meaning in this theory is always relational. $A$ by itself has no intrinsic properties; it is defined by its relations to $A^*$ and other objects. Even $I$ was defined only by a relation to itself. This adheres to the philosophical idea that all qualities are secondary to relationships. There is no “essence” floating beyond or behind the network of morphisms – the morphisms are what we know of the objects.

In a topos, this idea is reinforced by internal logic: an object’s subobjects (akin to its properties) are morphisms into the subobject classifier $\Omega$. Those ultimately trace back to arrows and objects relating things within the category. Hence, our model does not fall prey to an infinite regress of definitions – the regress stops at the initial tautology ($I$ exists because $I$ relates to itself). Everything else is built up from there. Mathematically, we ensure $\mathbb{U}$ has the structure to express all this. Being a dagger-compact closed monoidal topos, $\mathbb{U}$ can be thought of as a very flexible universe of discourse that generalizes Set (the category of sets) but also includes quantum-like structures. For example, the requirement of a dagger means we have an operation (denoted $\dagger$) acting as an involutive contravariant functor: for every arrow $f: X \to Y$, there is an $f^\dagger: Y \to X$ satisfying $(f^\dagger)^\dagger = f$ and $(g\circ f)^\dagger = f^\dagger \circ g^\dagger$. Physically, this gives us a notion of reversing processes, which parallels the existence of time-reversed solutions in physics or the idea that interactions are two-way (if $A$ influences $B$, there is a sense in which $B$ can influence $A$ – though not necessarily symmetrically or with the same probability).

The compact closed part (dual objects and evaluation/coevaluation) we’ve already discussed as providing a way to spawn new pairs from nothing. “Monoidal” tells us we have a sensible way to talk about joint systems ($A \otimes B$) and non-interacting combinations, much like having independent subsystems in physics. Finally, being a topos implies the category supports logical operations (and, or, not, etc., internally) and can interpret mathematical statements. This is helpful to incorporate logic and mathematics into physics – something we expect if the universe fundamentally computes itself. (In some sense, $\mathbb{U}$ could be the universe – a cosmos of all possible objects and relations, one of which is our concrete physical world seen as a subcategory or as phenomena within $\mathbb{U}$. But for this paper, we restrict attention to the subcategory modeling our world.)

To summarize this foundational stage: We have one starting object $I$ and one starting morphism $\mathrm{id}_I: I \to I$. This tiny self-referential loop is the spark that lights the fire. It encodes “Being = Self-Being”. Everything else – space, time, particles, forces, you reading this text – will be constructed by letting this spark ignite a relational wildfire.

\subsection{Iterative Growth: Morphisms That Beget New Objects}

How do we go from one self-related object to the richness of our Universe? The answer is iterative growth by adding relations, a process we call meaning assignment. At each “step” (conceptually, in logical time), the existing category acquires new morphisms (and possibly new objects) that consistently extend the web of relationships. Each new morphism is interpreted as answering a question or making a distinction that was not present before, thereby adding new “meaning” to the world. There are three guiding principles we impose on this growth, inspired by known physical and informational principles:

\begin{description}
\item[Maximal Symmetry:] The growth should preserve or maximize symmetries in the structure. Symmetry here means invariances or self-consistency constraints – for example, if two ways of relating objects are logically the same, the theory should treat them the same. In practice, this principle leads to the emergence of familiar symmetry groups (continuous or discrete) in the relational structure, because symmetric configurations are favored. Symmetry is tied to conservation laws and uniformity: it ensures no "element of reality" gets special treatment without cause. For instance, symmetric monoidal structure already implies some invariances (isomorphisms like $A\otimes B \cong B\otimes A$). As the network grows, maximal symmetry might manifest as the Lorentz symmetry of spacetime, gauge symmetries of forces, or permutation symmetries of particle families, all arising as automorphisms of the relational graph that the growth process does not break.

\item[Maximal Integration (Unity):] This principle is analogous to striving for high integrated information in the sense of Integrated Information Theory (IIT) of consciousness. IIT defines a quantity $\Phi$ that measures how irreducible a set of connections is – a high $\Phi$ means the system is “more than the sum of its parts”\cite{oizumi2014}. In our context, maximal integration means the relational network tends to form holistic, strongly connected structures rather than many disjoint pieces. Intuitively, the Universe doesn’t fragment into unrelated islands; it knits itself into a unified tapestry. We will see this principle at play in phenomena like entanglement (parts of the system are deeply interconnected) and in the existence of coherent law-like behavior across the cosmos (since everything is part of one unified structure, consistent laws apply universally). In more concrete terms, when new morphisms are added, there’s a preference for ones that tie subsystems together into larger complexes rather than isolate them. (This principle is speculative but can be motivated by the idea that a self-creating universe would favor creating stable, unified structures; a completely fragmented world couldn’t self-maintain meaning.)

\item[Maximal Entropy (Diversity):] Lastly, we incorporate a drive toward variety and proliferation of relations – akin to a maximum entropy production principle. While integration favors unity, entropy favors multiplicity. The interplay of the two yields complex but structured growth. In physical terms, this resembles time’s arrow: the system evolves toward more possible configurations (higher entropy) unless constrained by symmetry or integration. This principle encourages the creation of new distinct objects and relations (new degrees of freedom) to “fill out” what is allowed. One can think of this as the universe exploring all consistent ways of building relations, thereby “discovering” new states and particle types, etc., up to the point that adding more would violate a symmetry or reduce overall integration (making the world too disconnected or unstable). In other words, the universe is computing all that it can, but also staying self-consistent.
\end{description}

The above three principles – symmetry, integration, entropy – act somewhat like competing energies or optimization criteria for the growth of the category. The actual evolution of the network of morphisms is then akin to a computational process that at each step tries to maximize symmetry and integration, and increase entropy (diversity) as much as possible without violating the first two. This picture aligns with how physical systems often evolve: e.g., the expansion of the universe increases entropy, but fundamental symmetries (like charge, momentum conservation) remain; life evolved increasing complexity (integrated information) while still obeying the drive for entropy production. Let’s illustrate the growth with a simplified sequence:

\begin{enumerate}
\item[Step 0:] We have object $I$ and morphism $\mathrm{id}I: I \to I$. The “state” of the universe is trivial self-awareness.
\item[Step 1:] Invoke coevaluation: $\text{coev}{A}: I \to A \otimes A^*$ for some new type $A$. Now we have objects $A$ and $A^*$. We also automatically get $\text{ev}_{A}: A^* \otimes A \to I$. We interpret $A$ as a new “thing” and $A^*$ as its complementary thing, created as a pair. At this point, $A$ means “that which, combined with $A^*$, gives back void.” By itself, $A$ is just the potential to become something when related to $A^*$. We might say $A$ is an observer and $A^*$ the observed, or vice versa – the roles are not fixed. They are a dual pair.
\item[Step 2:] To generate nontrivial physics, we let morphisms proliferate. Perhaps we add a morphism $f: A \to A$ (an endomorphism on $A$) that is not the identity. This could represent a “state transition” or a property of $A$. By dagger, $f^\dagger: A \to A$ also appears (unless $f$ is unitary in which case $f^\dagger = f^{-1}$, its inverse). We might also add a morphism $g: I \to A$ (a “creation” of $A$ from the void, which in presence of the pair creation might correspond to choosing either the $A$ or $A^*$ half specifically). However, direct $I \to A$ might violate conservation (we can allow it only if accompanied by something into $A^*$ or another context; indeed $I \to A$ would pick a specific branch of $A\otimes A^*$, breaking a symmetry, so it might be suppressed unless justified).
\item[Step 3:] Add morphisms between $A$ and $A^*$. For instance, a morphism $u: A \to A^*$ could be added. This we can think of as giving $A$ a property that corresponds to $A^*$ or an interaction between them. Once $u: A \to A^*$ exists, by composition with $\text{ev}: A^* \otimes A \to I$, we get a composite $A \otimes A \xrightarrow{1_A \otimes u} A \otimes A^* \xrightarrow{\text{ev}} I$. This composite is a two-input morphism (one $A$ came from the left of $\otimes$, one from the ev pairing) yielding $I$ – essentially an interaction that annihilates two $A$’s using one $u$ as a bridge. Such a morphism could represent something like two $A$ particles annihilating into vacuum via an interaction $u$. In physics terms, if $A$ were a particle and $A^*$ an antiparticle, $u$ could be akin to a field that mediates annihilation.
\end{enumerate}

As more morphisms are added, the category’s structure becomes richer. Symmetry constraints will identify certain composites as equal, reducing redundancy. For example, perhaps $A$ and $A^*$ turn out to be isomorphic objects (that would mean matter and antimatter are essentially the same thing viewed in reverse, a symmetry often expected unless broken by some mechanism). Integration will encourage connecting $A$ and $A^*$ into one system rather than having them completely separate – maybe forming a single combined object or ensuring that any state of $A$ implies something about $A^*$. Entropy will drive us to consider adding a new kind of object $B$ if possible, along with its dual $B^*$, thereby creating richer interactions (like new particle species).

One can envision a rapidly expanding web: $I$ spawns $(A,A^*)$, which together or separately spawn $(B,B^*)$, $(C,C^*)$, etc. At each stage, all existing objects can relate to all others in new ways. The number of possible morphisms grows combinatorially with the number of objects, but not all are allowed – they must be consistent with previous ones and with our principles. The structure somewhat resembles a growing semantic network or a self-programming computer: it’s making “statements” (arrows) that either define new things or relate old things, constrained by logical consistency (the categorical identities like $f \circ \mathrm{id} = f$, etc.) and by symmetry/integration requirements (which we imagine as something like: if a relation can hold in two symmetric ways, include both or neither; if a set of relations would split the world into two disconnected parts, that’s disfavored, etc.).

At a more concrete level, it’s useful to draw a parallel between this growing category and a Feynman diagram or graph forming over time. Each object can be seen as a node, and each morphism as a directed edge or interaction. Initially we have one node with a self-loop. Then a pair of nodes connected by some link, etc. Over “time” (not physical time, but an algorithmic generation parameter), we add more nodes and edges. Eventually, a very large graph forms – in fact an ever-evolving one, since the process never stops. That large graph, we claim, can be identified with the physical history of the Universe: nodes correspond to degrees of freedom (fields, particles, systems) and edges to interactions or causal influences between them.

Key claim: If this relational-computation picture is correct, then running it should produce a structure isomorphic to our observed Universe (at least in its broad properties). In an ideal realization, we could compute the category’s growth and read off emergent quantities like “space dimensionality,” “particle mass ratios,” etc. In practice, that’s immensely difficult – it’s like trying to derive all of physics from first principles. However, we can already reason about some outcomes by using general arguments and simpler models, as we’ll do in the next section. To ground these ideas, let’s briefly foreshadow two notable emergent results that we argue arise naturally:

Why 3+1 Dimensions? In our framework, “dimensions” of spacetime correspond to independent directions one can move without immediately closing a loop. We find that a relational network naturally unfolds into three spatial and one temporal dimension as the only stable, maximally symmetric option. Fewer than 3 spatial dimensions don’t allow the complexity of relations needed (for example, in 1 or 2D, forces and stable orbits behave pathologically so complex structures can’t form\cite{tegmark1997}). More than 3 spatial dimensions allow too many independent relational degrees (force laws would behave differently and atoms and planetary orbits become unstable or non-closed)\cite{tegmark1997}. The growing network “freezes” out at 3 dimensions as a sweet spot where enough room exists for structures, but not so much that orbits fly apart or inverse-square forces no longer bind. (This aligns with anthropic reasoning by Carter, Tegmark, Barrow, etc., but here it’s a dynamical selection via maximal integration and symmetry – Section 3.2 will elaborate.)

Why 3 Generations of Fermions? The Standard Model of particle physics features three families of quarks and leptons. Why exactly three? Our model suggests an answer: at least three are needed for certain necessary relations (like CP-violation) to exist, but any more would produce redundant structure that gets trimmed by symmetry. In particular, with only two generations of particles, the category of flavor rotations has a determinant that is real – there is no complex phase to allow CP-violating processes. Only with three generations does a complex phase appear in the mixing morphisms, which is required to explain the matter–antimatter asymmetry in the Universe\cite{kobayashi1973}. (Historically, Kobayashi and Maskawa predicted a third generation for this reason\cite{kobayashi1973}.)

Our model’s integration principle says the Universe wants to achieve a state where there is an “irreducible” interaction generating matter over antimatter – CP-violation provides that, enabling integrated structure (like galaxies made of matter). Three families is the minimal number to achieve it. If we try to add a fourth generation, it seems we would introduce extra “flavor” relations that are largely similar to the first three (not offering fundamentally new asymmetries) and are heavily constrained by symmetry (e.g., anomaly cancellation, which in the SM already ties generations to color degrees). Thus, more generations would either mirror existing ones or break something (like electroweak precision constraints), so the growth process likely halted at three as a compromise between entropy (more particle types) and symmetry/integration (no new functionality beyond three).

The above are qualitative arguments, but we will back them with more detail in Section 3. For now, the main point is: the relational growth is not arbitrary – it is guided by principles that can produce specific outcomes matching reality. Before moving on, it’s worth noting that our approach bears some resemblance to algorithmic or computational theories of physics. We can think of the Universe as continually computing its next state based on current relations (somewhat like a cosmic cellular automaton). However, unlike a fixed lattice automaton (e.g., Wolfram’s hypergraph rewriting which applies a set rule repeatedly\cite{wolfram2020}), our “rules” are more like conditions (symmetry, etc.) that must be satisfied, and the actual rule may be “explore any new relation that doesn’t violate consistency and increases some goodness metric”. This is less deterministic than typical automata; it might even be nondeterministic or opportunistic, akin to how an evolving organism explores mutations. One could say the Universe is performing a search in the space of possible relational configurations for those that are self-consistent and fecund (rich in structure). The amazing fact – if our theory holds – is that this search naturally yields known physics. The next section will demonstrate this by showing how layers of physical law emerge one after another in the relational model.

\section{From Relations to Physics: Emergent Layers of Reality}

Starting from the abstract mechanism of Section 2, we now outline how familiar physical properties arise. We will go through layers of organization, from the microscopic (quantum) to the cosmic, and even touch on life and mind as top-level emergent phenomena. The goal is to show that this single framework can unify a wide range of seemingly disparate features. Throughout, we indicate how each layer could be “read off” from the relational category and compare to known physics.

\subsection{Quantum Physics as Category Theory in Action}

It is no coincidence that our chosen categorical structure (dagger-compact, etc.) matches the formal structure of quantum theory – we intended that. Abramsky and Coecke, Selinger, and others have shown that finite-dimensional quantum mechanics is naturally expressed in dagger-compact categories\cite{abramsky2009}. In our model, quantum behavior isn’t assumed; it emerges from the relational rules. Let’s see how:

\begin{description}
\item[States and Superposition:] An object $A$ in the category can be thought of as the type of a quantum system. Morphisms $I \to A$ represent states of that system (since $I$ is like the trivial input – a state preparation yields an $A$). Multiple distinct morphisms $I \to A$ can exist, and they can in general form a superposition. In a topos, we might not have linear combination by default, but since we can model Hilbert spaces, imagine that the hom-set $\text{Hom}(I,A)$ forms a vector space corresponding to the state space of $A$. The identity morphism $\mathrm{id}_A: A \to A$ acts like a delta preserving normalization. Superposition reflects the idea that the category can encode multiple possible relations until an interaction (morphism with another system) picks one.

\item[Unitarity and Reversibility:] Dagger gives us a notion of adjoint. If a morphism $U: A \to B$ has an adjoint $U^\dagger: B \to A$ and $U^\dagger \circ U = \mathrm{id}_A$, $U \circ U^\dagger = \mathrm{id}_B$, then $U$ is unitary – a reversible transformation. We expect physical time evolution or symmetry operations to be unitary (in quantum mechanics). The category inherently can host such unitary morphisms (they are just isomorphisms in $\text{Hom}(A,B)$ respecting the dagger). Thus, dynamics in an isolated system is reversible in this emergent picture – irreversibility only enters when integrating out parts of the system (e.g., an arrow representing a measurement that discards some information).

\item[Entanglement:] Because we have a tensor product, we can form composite systems. A morphism $I \to A \otimes B$ represents an entangled state of $A$ and $B$. The compact structure’s coevaluation map $\text{coev}: I \to A \otimes A^*$ is actually the canonical maximally entangled state between $A$ and its dual $A^*$. In physics, this is like the $|\Psi^-\rangle = \frac{1}{\sqrt{d}}\sum_i |i\rangle \otimes |i^*\rangle$ state (Bell pair) if $A$ is $d$-dimensional. Thus the existence of duals guarantees entanglement is a built-in feature. The growth process almost certainly produces entangled states because coevaluations are used to spawn new objects, and those are entangled by construction. This aligns with the idea that the early Universe was in a highly entangled state (as often considered in cosmology or in holistic quantum interpretations).

\item[Uncertainty and Non-commutativity:] In category terms, having two morphisms $p, q: A \to A$ that do not compose interchangeably ($p\circ q \neq q\circ p$) is a kind of non-commutativity. This is expected if those morphisms represent two physical observables or transformations that can’t be simultaneously diagonalized (like position and momentum, or two Pauli matrices). The uncertainty principle arises because not all morphisms share common eigenstates. Our framework will naturally have non-commuting morphisms whenever the relational web allows feedback loops or competing relations. For example, $A \to A$ via one path vs another path might differ if one goes through $A^*$ and back, etc. In short, non-commutative algebra emerges from the network of relations, and with it the uncertainty principle: the order in which relations (measurements) are applied matters, limiting knowledge.

\item[Particle-like excitations:] If the category has a symmetry corresponding to translations in an emergent space (see next subsection), then by Noether’s theorem (which we expect to hold since symmetry in the category leads to conserved quantities or invariants in morphism composition), there will be quantized modes. These modes are field excitations or particles. For instance, if object $A$ corresponds to a field, a state $I \to A$ could be a single-particle state. Because our model doesn’t start with a spacetime background, we won’t derive specific masses immediately. But we anticipate rest masses appear as invariants related to loops in the category (closed morphism cycles that can’t be shrunk – those give something like a phase $e^{-i mc^2 t/\hbar}$ over a time translation).

\item[Gauge symmetries:] The relational growth is supposed to maximize symmetry, which implies gauge symmetries (redundant descriptive degrees of freedom). In category terms, a gauge symmetry might manifest as a natural transformation of the identity functor on a subcategory of states – essentially a way to systematically change phases or basis of certain objects without affecting observable relations. For example, if there’s an internal symmetry on object $A$ (like $A$ has an automorphism group), that could correspond to a $U(1)$ phase rotation (for electromagnetic charge) or $SU(2)$, $SU(3)$ etc. In other words, because $\mathbb{U}$ is so general, it contains many automorphisms; the physical content lies in isomorphism classes of relations. So transforming $A$ by an automorphism might not change the isomorphism class of a state, meaning it’s a gauge symmetry. We expect the familiar gauge groups of the Standard Model to arise as automorphism groups that maximize symmetry while allowing the necessary distinct interactions. (E.g. $U(1)\times SU(2)\times SU(3)$ could be automorphisms on different “sectors” of the category corresponding to different interaction types.)
\end{description}

In summary, the quantum layer emerges naturally: our category has objects and morphisms that behave exactly like quantum states and operators. The famous paradoxes and features of quantum mechanics (superposition, entanglement, measurement collapse) are interpreted cleanly as properties of a relational system: superposition is just multiple possible relations, entanglement is a relation that cannot be factored, and collapse is the creation of a new relation (observer–observed link) that selects one branch and integrates the information into the observer (consistent with relational QM where the state is observer-dependent\cite{rovelli1996}).

\subsection{Spacetime and Gravity from Relations}

How do space and time arise from an initially atemporal network of relations? The key lies in identifying patterns in the relational graph that behave like geometry. We expect an approximate notion of spacetime to emerge as a coarse-grained description of the information flow in the category.

\begin{description}
\item[Space as Connectivity Graph:] Consider a large number of objects $A_i$ (think of them as degrees of freedom, like field modes or particles). Suppose there are morphisms $A_i \to A_j$ for many pairs $(i,j)$. We can form an undirected graph of connectivity (ignoring direction for the moment, or considering that if either $A_i\to A_j$ or $A_j\to A_i$ exists, there is some adjacency). This graph encodes who interacts with whom. If this graph has the property that it can be embedded in a 3-dimensional manifold such that adjacency corresponds to spatial neighbor relations, then we can interpret the graph as a discretization of 3D space. A more refined picture: the category might form something akin to a simplicial complex or topological space via relations – e.g., an object with many mutual relations could represent a region of space. The reason we expect specifically 3 dimensions ties back to symmetry and stability: in a network, planar graphs (graph that can be drawn in 2D without edges crossing) are too restrictive to accommodate all necessary relations (for instance, you can’t have a fully connected network of more than 4 nodes in a plane without crossings). Higher than 3D, graphs become too connected (gravity in >3 dims has no stable orbits). Precisely, only in 3 dimensions can inverse-square force laws produce stable bound orbits and can 3D topologies allow nontrivial links without infinite connectivity issues\cite{tegmark1997}. So the growth naturally favors a 3D-like relational structure. In the category, this might manifest as the existence of three generating independent directions for how relations can extend, which correspond to $x,y,z$ axes locally.

\item[Time as Causal Order:] Now, consider the directed nature of morphisms: we get a partial order of events (if an arrow goes from $A$ to $B$, we say $A$ “precedes” $B$ in causal sense). The category as a whole has arrows that can be composed to form sequences, which define a causal order (especially if we restrict to relevant morphisms like propagation of influences). We identify this partial order with time. Each morphism is like an event, and if $X \to Y$ exists, event $X$ is in the past of event $Y$. Importantly, our model does not assume a global synchronized time parameter; time is emergent as an order structure (similar to causal set theory’s view of time as the partial order of events\cite{bombelli1987}). Because we have the arrow of time given by the growth (new morphisms add arrows predominantly forward), we align that with the forward direction of physical time (entropy increasing direction). Dagger symmetry means for every process we can mathematically imagine a reverse, but those reverse processes won’t occur in bulk because that would decrease entropy.

\item[Spacetime Manifold and Lorentz Symmetry:] As the network becomes large, there may be so many events that a continuum approximation becomes useful. At that point, one can try to fit a smooth manifold that approximates the connectivity (space) and partial order (time). The result would be a spacetime manifold. Our model’s maximal symmetry principle strongly suggests the emergent spacetime will obey Lorentz symmetry (or some generalized relativity). The reason is: if no preferred frame is built into the relational rules, the system of relations should be invariant under the symmetry group of the distance/causal structure. In a Minkowski-like spacetime, that’s the Poincaré group (Lorentz transformations and translations). Indeed, approaches like dynamical triangulations of spacetime find that if you don’t put in anisotropies, the emergent large-scale geometry is usually 4D and approximately Lorentzian. We similarly expect an isotropic, homogeneous emergent spacetime – basically the Cosmological Principle emerges because any biases would violate symmetry unless forced by other aspects (like initial conditions). So in effect, the category’s large-scale limit is a 3+1 dimensional Lorentzian spacetime. Distances and durations relate to how many intermediate morphisms connect two objects or events. If many independent composition steps are needed, that might correspond to a long time or distance. Light cones emerge as the sets of points $Y$ that can be reached from $X$ via a morphism chain not exceeding a certain length (number of steps corresponds to time, if each step is like one tick). If our category also carries something like a quantum state amplitude for each morphism, the speed of information transfer might be limited by something (maybe related to the average time for a chain to percolate – this could yield the speed of light $c$ as an emergent constant setting an upper bound on relational influence propagation).

\item[Gravity via Curvature of Relations:] In general relativity, gravity is the curvature of spacetime geometry. In our model, curvature comes from inhomogeneous relational density. If the network of relations is uniformly distributed, space is flat. If there’s a region with a high density of morphisms linking things tightly (representing mass-energy concentration), it effectively pulls neighboring relations closer, altering shortest connection paths – this mimics curvature. We can quantify it: think of a massive object as a sub-category that has a huge number of internal morphisms (integration) and thus requires more “effort” (or steps) for other objects’ relations to go through that region. The effective result is that geodesics (paths of stationary phase or maximal integration) bend toward the region – just like gravity attracts matter. This heuristic aligns with proposals like emergent gravity by Verlinde, where gravity is an entropic force caused by gradients in information density\cite{verlinde2011}. More concretely, one could map certain categorical invariants to Einstein’s field equations. For example, the presence of a large tangled cluster of relations (like a black hole) might correspond to a region that the rest of the category sees as essentially a single object with a missing interior – analogous to a horizon. In fact, the compactness of our category with evaluation/coevaluation suggests something like the holographic principle: information about an object $A$ can be encoded on its boundary relations ($A \otimes A^* \to I$ maps).

A black hole in our model would be an object (or set of objects) whose internal relations are maximally integrated (high $\Phi$) such that no external morphism can fully probe inside – all you see is the boundary summary (like mass, charge). The entropy of a black hole ~ area/4 (in Planck units) could be interpreted as the number of fundamental morphisms needed to describe the horizon’s relation to the rest of the world\cite{bekenstein1973}. While we won’t derive the Bekenstein-Hawking formula here, the model is amenable to it since the category can treat surfaces as special objects (in a topos of presheaves, a horizon could be a subobject classifier mapping that effectively counts states on a boundary). Finally, about dark matter: Our model provides a novel interpretation. If not all relational structures are symmetric or integrable, some may behave like an extra curvature that is not accompanied by visible matter. For instance, there could be “off-diagonal” relations or loops in the category that contribute effective gravity but don’t interact via usual particle morphisms with regular matter. This echoes Verlinde’s idea that dark matter is not a particle but an emergent additional gravity effect from information geometry\cite{verlinde2016}.

In our terms, dark matter is relational structure that’s not integrated into the network of quantum exchanges but still curves the web. One way to test this is to see if the theory predicts subtle deviations from the cold dark matter particle model – e.g., a certain relation between dark matter distribution and entropy or the inability to find a dark matter particle in experiments (since there is none, just unseen relations). If future galaxy observations show gravity deviating exactly as some entropic models predict, it would support this view. Current tests are inconclusive\cite{verlinde2016}, but our framework is flexible enough to incorporate either outcome.
\end{description}

\subsection{The Particle Zoo and Interactions}

Delving deeper into the emergent particle physics: The fundamental particles (quarks, leptons, gauge bosons, Higgs, etc.) can be understood as irreducible objects and morphisms in the category. “Irreducible” here means they correspond to simplest building blocks that can’t be described as composites of others in our relational language (at least at low energies). This is analogous to simple representations of a symmetry group in usual particle physics.

\begin{description}
\item[Fermions and Antisymmetry:] Fermions (electrons, quarks, etc.) obey the Pauli exclusion principle – no two identical fermions can occupy the same state. In our category, this can be traced to the braiding symmetry of the monoidal structure. If our category is symmetric monoidal, swapping two objects $A\otimes A$ yields the same composite. But if we attach a sign to that swap (making the symmetry graded), we get antisymmetry: swap $A$ and $A$ picks up a factor -1. This kind of structure can be imposed to model fermionic relations (it’s like working in supercategories with $\mathbb{Z}_2$-grading). The result is that morphisms which would exchange two identical $A$ objects cancel out if they are symmetric, forcing antisymmetric state combinations. This is the categorical way to get Fermi statistics. The presence of such a grading likely emerges from integration principle: if two objects were totally symmetric and could sit on top of each other, they would effectively become one object (reducing integration). To preserve distinct identities, identical objects might acquire a rule that too much overlap is disallowed – which is Pauli’s principle in essence.

\item[Bosons and Forces:] Bosons (force carriers like photons, gluons, W/Z bosons) correspond to morphisms that mediate interactions between fermionic objects. For example, an arrow $A \to A$ that actually factors as $A \to I \to A$ (via some intermediary $I$ or another object) could indicate emission and absorption of a particle. A better representation: a morphism of type $A\otimes B \to C\otimes D$ could be an interaction vertex where particles $A$ and $B$ scatter into $C$ and $D$. In the category, such a morphism exists if relations allow those combinations. The gauge bosons would be associated with symmetry transformations: e.g., a photon is associated with a $U(1)$ symmetry, meaning there’s an invariant phase factor – the photon creation can be seen as the generator of that phase (a morphism that when composed with some others causes a phase shift).

\item[Higgs mechanism:] The Higgs field provides mass by its vacuum expectation. In our model, a Higgs-like object would be a special object $H$ with a morphism $I \to H$ (meaning $H$ has a nonzero default state filling the vacuum) and interactions linking $H$ to fermion objects ($H \otimes F \to F$ effectively, giving mass to $F$ by dressing it with an $H$ relation). The existence of such an $H$ might be required for symmetry breaking in the category: e.g., to break electroweak symmetry (distinguish electromagnetic $U(1)$ from the unified electroweak), the growth process finds it favorable to add a morphism that isn’t symmetric under the full group, reducing it. This ties to a preference for lower free energy (the Higgs vacuum gives a lower potential state).

\item[Generation structure:] As already discussed, three generations could emerge because of the need for CP-violation and possibly anomaly cancellation. The category may produce three copies of similar relation structures ($e,\mu,\tau$ families) because entropy likes variety, but beyond three integration or symmetry might fail. For instance, the anomaly cancellation in the Standard Model (cancellation of gauge anomalies between quarks and leptons in each generation) might be telling us that generation number and charge assignments are not arbitrary but locked by consistency. In a relational sense, an anomaly is an inconsistency in trying to assign a global rule to all relations – having exactly three families times their charges sums to zero anomaly, which could be a requirement of the topos’s internal logic (the “sum of charges = 0” for the whole Universe as a global consistency law). Notably, the CKM matrix (which describes quark mixing between generations) contains a complex phase $\delta$ ~ 1.2 radians that signals CP-violation. Our model predicted qualitatively that a nonzero phase arises only with 3 generations; any additional generations, the effective phase might get diluted or new cancellations appear. So three is the maximal number that yields an irreducible CP-phase – any more would either reintroduce a possibility to rotate phases away or add redundant copies. This could be tested if someday a 4th generation were found – it likely won’t be, and our model aligns with that expectation.

\item[Constants and Parameters:] Many fundamental constants (c, Planck’s constant, fine-structure constant α, etc.) appear as parameters in equations. In our framework, they would emerge as ratios of scales or coupling strengths in the relational web. For example, the speed of light c would be the maximum rate of information propagation in the network – essentially set by the “locality” of the relational graph (maybe something like each morphism or a chain of $n$ morphisms corresponds to a certain fixed time tick, so that no influence can travel faster than 1 edge per tick, making a finite invariant speed). The Planck length might appear as the smallest meaningful length where the discrete nature of the category becomes noticeable (maybe the scale at which a single morphism or unit relation can occur – if each fundamental morphism has a sort of unit action, then the Planck scale is where spacetime’s smoothness breaks). Our initial statements noted these values: $c \approx 3\times10^8\ \text{m/s}$, $l_P \approx 1.6\times10^{-35}\ \text{m}$; we do not derive them here from first principles, but the hope is a mature version of the theory would derive them by considering the density of relations at the fundamental level and the overall size/age of the Universe (the Hubble scale ~$c/H_0$). In fact, Hubble’s constant $H_0$ might be interpretable as the “growth rate” of new relations over cosmic time – essentially how fast the network is adding links relative to existing ones. The observed $H_0 \approx 67.4\ \text{km/s/Mpc}$ could correspond to an entropy production rate in the network. If the network doubles its number of relations on a timescale, that could tie to an exponential expansion at early times (inflation) and then slow expansion as symmetry/integration constraints kick in (yielding the current $H_0$).

\item[Entropy and the Arrow of Time:] In thermodynamics, entropy tends to increase. In our model, this is just the statement that the relational network tends to add more morphisms (diversify) over time. Early on, when few objects existed, adding a new relation made a big relative change (like early Universe had huge jumps in complexity). Later, many relations exist and new additions cause smaller fractional increases, so the entropy production rate slows – this could correlate with cosmic expansion slowing after an initial burst. The arrow of time is simply the orientation of the category’s growth – we remember the past (fewer relations) and not the future (which doesn’t exist yet) because information is integrated in one direction. Interestingly, this also explains why we can’t reverse time easily: that would require deleting relations in a perfectly symmetric way, which the integration principle forbids (once information is integrated, you can’t just segregate it without adding even more relations to keep consistency, which ironically increases entropy further).
\end{description}

We see that qualitatively, the particle physics and cosmic physics in our model are deeply intertwined – a success of unification. Parameters that we normally have to fine-tune (like the electron/proton mass ratio, the strengths of forces) might in principle be calculable by analyzing the optimal relational configuration that meets all principles. Admittedly, in this paper we are mostly providing plausibility arguments, but they are backed by known science where possible. For example, earlier we used an anthropic argument for 3 dimensions\cite{tegmark1997} and a known Standard Model fact for 3 generations\cite{kobayashi1973}. We also referenced how emergent gravity from entanglement is an active research topic\cite{vanraamsdonk2010}, giving credence that a relational foundation can yield general relativity with extra “dark” effects\cite{verlinde2011}.

\subsection{Life and Consciousness: The Highest Emergent Layers}

One special appeal of a “universe of relations” is that mind and matter are not fundamentally different kinds of stuff in this view – both are patterns in the relational web. Traditionally, physics stops at atoms and molecules, handing off to biology for life phenomena, and then to neuroscience or cognitive science for consciousness. But if our framework is truly universal, it should accommodate those phenomena seamlessly as higher-level emergents when certain complex structures appear.

\begin{description}
\item[Life as Self-Replicating Sub-networks:] A living organism could be defined as a subsystem of the Universe that maintains and replicates itself, using flows of energy/information. In the category picture, life would correspond to a subcategory that is dynamically closed and autocatalytic – meaning the morphisms within it produce new morphisms that largely stay in that subcategory (the organism’s internal processes) and can generate a copy of that subcategory under the right conditions (reproduction). The integration principle corresponds here to what in biology is the drive for homeostasis and functional integration (organs forming a cohesive organism). The entropy principle corresponds to metabolism and dissipation – living systems increase entropy in their environment to sustain their structure. Our model’s framework can, in principle, host a network-of-networks where a particular network pattern (DNA, etc.) encodes the replication instructions to create similar relational patterns. While we obviously won’t derive the genetic code here, the hope is that in a fully fleshed theory, one might see why chemistry (the relational rules of molecules) permits self-replicating patterns – possibly linking back to category theory via something like chemical reaction networks as category theoretic Petri nets. Indeed, researchers have applied category theory to chemistry and biology for this reason: it handles complex interactions well.

\item[Consciousness as Integrated Information:] Perhaps the most intriguing emergent is consciousness. Integrated Information Theory (IIT) posits that consciousness corresponds to a system with a high value of $\Phi$, meaning a lot of irreducible integrated information\cite{oizumi2014}. This fits snugly with our integration principle. In our model, a conscious observer is simply a subsystem (object, or collection of objects) that has evolved to have maximally integrated relations within itself – it’s effectively strongly self-referential in a rich way (far beyond the trivial $I \to I$, it’s more like a complex web that has many feedback loops but can’t be reduced to smaller networks). Such a system can assimilate information from its environment (create morphisms from many external objects into itself) and make unified sense of it – exactly what an observer does by “measuring” the world.

The principle of maximal integration implies that, as systems grow in complexity, some will hit very high $\Phi$ and become observers by nature (they start having experiences from the intrinsic perspective). This is speculative but IIT provides a quantitative handle: one could imagine computing $\Phi$ for network configurations in the category. A threshold $\Phi$ value (say $\Phi \gtrsim 1$ for minimal experience, or a higher number for something human-level – one source suggests humans might have $\Phi$ on the order of $10^{14}$ bits, but Tononi’s theory is not yet able to compute that for brains). In our simplified earlier attempts, we guessed a threshold $\Phi \approx 9$ in some units for meaningful self-awareness; that was more illustrative than rigorous. The key point is, consciousness is not something we need to add separately – it’s a natural endpoint of increasing relational complexity, where a part of the Universe can model the rest of the Universe within itself to some degree (hence “observe” it). This resonates with Wheeler’s participatory universe: the Universe observing itself through us\cite{wheeler1989}.

It’s worth noting that if consciousness is an emergent property of high integration, then it might appear gradually in evolution (which it likely did), and our model qualitatively supports that – as the relational network representing Earth’s biosphere became more integrated (complex nervous systems forming), at some point cognitive observers appeared. This also means any sufficiently integrated system, even artificial, could be conscious. One could test this by trying to apply IIT on advanced AI architectures – indeed some are doing that. Free will and agency could be described as the ability of a subsystem to effect changes on other parts of the network in a way not predictable by simpler subsystems. Since our model has inherent unpredictability (quantum uncertainty, emergent chaotic dynamics), a highly integrated system might leverage that to produce actions not derivable by external observers – which from the inside are experienced as choices.

While these ideas are far from settled science, it’s exciting that a single framework of relational interactions could span from quantum physics to human consciousness. Traditional physics might scoff at mixing these, but as we push for a complete understanding of reality, ignoring the role of observers (which quantum mechanics taught us is important) is not an option. Our approach requires observers to be part of the system (no external classical observer was ever assumed; all measurements are interactions inside the category). Thus, subjects and objects are on the same footing. This holistic stance might help resolve longstanding dualisms.
\end{description}

\section{Comparison with Other Emergent Reality Approaches}

The idea that spacetime and physics emerge from something more fundamental has gained traction in various forms. Here we compare our category-theoretic relational model with two prominent frameworks: causal set theory and Wolfram’s hypergraph project. We highlight similarities (all aim for minimal assumptions, relational ontology) and differences (our approach emphasizes algebraic structure and internal logic, providing certain advantages).

\subsection{Causal Set Theory vs. Category of Relations}

Causal Set Theory (CST) posits that spacetime at the deepest level is a discrete set of events with only one relation: a partial order $\prec$ indicating causality (which event is before another)\cite{bombelli1987}. Essentially, it’s a sparse ontology – no distances or fields, just ordering and counting. This simple structure automatically ensures Lorentz invariance on large scales (since only order is fundamental, not an embedding in $R^n$), and it provides a possible quantum gravity path by summing over causal sets. Comparatively, our category model can be seen as an “enriched” causal structure. We also have a causality notion (morphisms direction), but we include types of relations and additional symmetries. In CST, two events either are related or not; in $\mathbb{U}$, there can be many different morphisms connecting two objects representing different kinds of interaction. So one could say our theory reduces to something like a causal set if you forget all the internal details of morphisms and only keep track of the causal partial order of events. Advantages of our approach:

\begin{itemize}
\item We incorporate quantum aspects from the start (CST struggles to include quantum field details – one has to put fields on the causal set in an ad hoc way later). By using a category that mirrors quantum Hilbert spaces and processes\cite{abramsky2009}, we unify spacetime and quantum descriptions.
\item We allow for rich internal symmetries (like gauge groups) naturally, because morphisms can carry group representations. In a causal set, introducing gauge symmetry or charge is nontrivial – you need extra labels on elements or additional structure.
\item Our model has a notion of logic and information (topos property) built-in, making it potentially easier to discuss things like the entropy or truth values of statements about the structure. Causal sets are primarily combinatorial and don’t directly speak to logical propositions.
\end{itemize}

However, with added richness comes added complexity. CST’s strength is its simplicity and thereby more straightforward computations (e.g., counting number of elements ~ volume, looking at order-theoretic invariants ~ curvature). Our model, being much richer, is harder to analyze without approximations.

If we restrict to just the causal order, both approaches yield an emergent 3+1D spacetime with Lorentz symmetry (CST requires a “Hauptvermutung” that manifold-like causal sets dominate; our approach similarly relies on maximal symmetry to get Lorentz invariance). Both predict some discretization of spacetime at small scales (CST has a discreteness scale, our category presumably leads to a Planck-scale network granularity). One could even embed a causal set into our category: take each event as an object and include a morphism for each causal relation. That subcategory would essentially be a thin category isomorphic to the causal poset. But in our model, we can have thick categories with many morphisms between objects, representing more physics. In summary, we extend causal sets by adding what’s needed to also handle the Standard Model and quantum processes, not just spacetime. The cost is complexity; the benefit is unification.

\subsection{Wolfram’s Hypergraph Model vs. Category Topos Model}

Stephen Wolfram’s Physics Project proposes that the universe is a kind of evolving hypergraph\cite{wolfram2020}. A hypergraph is like a network, but edges (hyperedges) can connect more than two nodes at a time. The model starts with some simple hypergraph and applies a fixed rewrite rule repeatedly: e.g., a rule might say “whenever you see a certain sub-hypergraph pattern, replace it with another pattern.” By exhaustive application, an extremely complex graph emerges. Wolfram and colleagues have shown that with appropriate rules, something resembling relativistic space, quantum branching, etc., can appear. Notably, they get Lorentz invariance statistically from something called causal invariance (different orders of applying rules yield equivalent causal graphs) and even derive Einstein’s field equations in some continuum limit scenarios. It’s an empirical, computational approach: choose a rule and simulate. Our category model shares the philosophy of minimal initial structure and emergent complexity\cite{wolfram2020}. But there are differences in methodology and scope:

\begin{itemize}
\item Determinism vs. Open-Ended Growth: Wolfram’s model uses a fixed rule, which is essentially a predetermined algorithm for the universe. Given an initial state and the rule, everything is determined (though due to many possible update orders, it feels nondeterministic like quantum – but presumably in principle it’s determined in the global picture). Our model does not specify a single rule; rather, it sets up conditions (symmetry, integration, entropy) that allow many possible “micro-rules” or events to occur. It’s more like an optimization or satisfaction process than following one algorithm. This might better capture the flexibility of physical law – our laws themselves could be emergent rather than hard-coded. However, one could say this makes our model underspecified – one needs a more concrete mechanism for how exactly new morphisms are chosen to be added. Wolfram’s clarity is an advantage there.
\item Hypergraph vs. Category: A hypergraph is basically a set of relations (each hyperedge is a relation among some nodes). A category is also a set of relations (morphisms) with extra algebraic structure (composition). In fact, a category can be seen as a type of directed hypergraph with composition laws. Interestingly, Wolfram’s technical documents note that their models can be viewed in different ways – combinatorial, functional, categorical, etc., as abstract rewriting systems\cite{wolfram2020}. So there is a connection: one can encode a hypergraph rewrite rule as a pushout in a category of graphs, for example. Our model might be viewed as a very generalized rewriting system where the “rules” themselves can evolve. We place emphasis on universal properties (like being a topos) whereas Wolfram fixes a specific substitution rule.
\item Inclusion of Quantum Mechanics: Wolfram’s approach initially struggled with quantum theory, but they introduced the idea of a multiway graph – effectively branching to represent superpositions. They then use an observer’s branch to induce apparent collapse, etc. This is somewhat ad hoc (though they claim it recovers the Feynman path integral). Our model inherently has quantum logic via the dagger-compact structure, which is arguably more elegant. It doesn’t need to manually branch – quantum interference is just built into the morphism algebra.
\item Experimental Testability: Both approaches are bold and not immediately empirically distinguishable at everyday scales. Wolfram’s model might produce subtle predictions like dimensionality change at small scales or preferred graph structures – but it’s hard to connect to experiments until more is derived. Our model, being even more abstract, might seem less testable. However, by linking to established physics (e.g., explaining known constants or phenomena differently), we open new avenues: for instance, if dark matter is relational curvature, we might see deviations from particle dark matter predictions in galactic behavior\cite{verlinde2016}. Or if the universe is self-computing, maybe we’d detect signs of discrete information processing (like holographic noise or limits on information density). There are experiments like the Holometer and upcoming quantum gravity detectors that try to see if spacetime is discrete. Both our approach and Wolfram’s would likely predict some discreteness at Planck scale – so far undetected.
\item Advantage of Category-Theoretic Approach: The main advantage we claim is conceptual unification and mathematical rigor. Category theory provides a unifying language for logic, math, and computation, whereas hypergraphs are a specific data structure. In our model, we don’t need separate stories for “how space emerges” vs “how quantum emerges” vs “how computation emerges” – it’s all the same story: the growth of a categorical universe. This might make it easier to connect to existing mathematics (for instance, we can draw on decades of categorical logic, or use functors to compare models). It also integrates the role of the observer/subject more naturally (topos theory has been used to address the “perspective problem” in quantum mechanics\cite{doering2008}, while hypergraphs treat the observer as outside unless you embed them as a subgraph artificially).
\end{itemize}

One can also mention computational efficiency: A potential drawback of our approach is that simulating a growing category with a lot of structure might be harder than simulating a hypergraph. Wolfram’s team can brute force some rules on supercomputers to see results. Ours might require reasoning with higher-level constraints rather than raw simulation (or finding clever ways to simulate category growth).

\subsection{Other Approaches and Context}

There are numerous other related efforts: spin networks/loop quantum gravity (graph states of spacetime geometry), matrix models (like ADS/CFT emergent space from large N matrices), informational and holographic principles (Bekenstein bound, entropic gravity). Each has elements of relational thinking (e.g., in loop quantum gravity, spin network links are the fundamental relationships that give space area/volume quanta). Our framework is in spirit closest to the idea of the “Universe as a quantum code or computation”. For example, quantum error-correcting codes have been invoked to explain the emergence of spacetime in holography – the code subspace’s structure yields a geometric dual. One could conceive of our category as implementing a massive error-correcting code: the consistency (symmetry) conditions are like parity checks ensuring the “code” (physical law) is self-correcting, integration ensures the code is not factorizable (so it encodes global logical info), and entropy means it uses up the coding space extensively. Another is John Wheeler’s idea of a participatory universe and decoherence/quantum Darwinism (where the classical world emerges from quantum through redundancy of information spread into environment). Our model also has that flavor: as relations proliferate, certain records (morphisms that copy information into many parts) become effectively immutable (classical facts). In summary, the category-theoretic approach distinguishes itself by:

\begin{itemize}
\item Starting from almost nothing except self-consistency, whereas some others start from a bit more (like a specific rule or a specific set of elements).
\item Embracing the full structure of known physics (quantum mechanics, relativity, symmetry, computation) in one formalism, rather than adding these in separate modules.
\item Being extremely general (topos theory could technically model universes with different logical laws – it’s a very broad church). This is both a strength and a challenge: as Peter Woit commented on topos approaches, they risk being so general it’s unclear if they make concrete predictions\cite{woit2008}. We mitigate that by incorporating physical guiding principles to narrow it down.
\end{itemize}

\section{Predictions and Tests of the Theory}

A theory of everything, to be credible, must not just retrodict known facts (explain them after the fact) but also predict new facts that can be checked. Our model, admittedly still forming, suggests several potential predictions or avenues to test:

\begin{itemize}
\item No Fourth Generation or Unobserved Particles Beyond the Standard Model (unless needed for integration): Because adding extraneous fundamental objects is not favored unless they solve a problem (like adding the Higgs solved electroweak symmetry breaking), our theory implies the Standard Model’s particle content is pretty much complete (three generations, forces as known, perhaps right-handed neutrinos if needed for neutrino mass, etc.). So far, collider experiments (LHC) have indeed not found a fourth-generation quark or lepton, nor any new force carriers or superpartners in the accessible energy range. This aligns with our model’s view that the Universe doesn’t add “unnecessary” relations. A surprise discovery of, say, a fourth-generation lepton at LHC or a new Z' boson would challenge the minimality aspect – though one could argue maybe they serve a purpose (like dark matter might be a new particle, which our theory instead explains as emergent curvature, so it predicts no WIMP particle).
\item Dimensional Stability: Our model predicts the dimensionality of space won’t change with scale (unlike some quantum gravity theories that predict effective dimension dropping to 2 at Planck scale). Here, 3D is maintained from large to small scales because it’s rooted in fundamental relational structure. If future experiments (like cosmic ray observations or gravitational wave propagation) hinted at dimension running with scale, that might contradict our assumption (or need explaining via something else).
\item Relational Dark Matter Effects: If dark matter is not particles but a property of the network, we might observe phenomena that particle dark matter wouldn’t easily explain. For instance, there might be a specific relationship between dark matter distribution and baryonic matter entropy or distribution. Emergent gravity models predict something like the MOND-like behavior (modified Newtonian dynamics) at certain accelerations\cite{verlinde2016}. Recently, some galaxies’ rotation curves and lensing behavior suggest a close coupling between visible and dark components (so-called Renzo’s rule or the Radial Acceleration Relation)\cite{mcgaugh2016}. Our theory could naturally account for that since dark “matter” isn’t independent stuff, it’s an extension of the geometry shaped by the visible mass distribution\cite{verlinde2016}. If upcoming surveys (LSST, Euclid) find that dark matter behaves precisely as an auxiliary effect (maybe failing to appear where it “shouldn’t”), that would support emergent views. If, conversely, dark matter is directly detected as a particle (WIMP detection or production), then our interpretation would need revision (though perhaps those particles themselves could be seen as just one way the network organizes curvature).
\item Black Hole Information/Holography: Our model implies black hole evolution is unitary (since the underlying category is). There is no loss of information; it's all in the relational structure (perhaps encoded on the horizon relations). This is in line with the holographic principle and what string theory/AdS-CFT suggest. If one day a novel experiment (maybe observing subtle correlations in Hawking radiation or some sign of information release) confirms unitarity, that would gel with our view. We also predict something like the Bekenstein-Hawking entropy formula conceptually: the area of a black hole (in Planck units) is proportional to the number of fundamental relation units at the horizon. If a quantum gravity experiment (e.g., gravitational wave echoes or relic neutrinos) ever reveals deviations from classical black hole predictions that align with a discrete area spectrum, that would be a win for these ideas.
\item Quantum-Consciousness Links: This is highly speculative, but if consciousness correlates with integrated information, and if that in turn ties into quantum entanglement in the brain (some proposals exist, though controversial), one might look for a signal. For example, if experiments on brain organoids or AI systems measure $\Phi$ and find a sharp jump when certain network complexity is reached (like the prediction of a threshold\cite{oizumi2014}), that’s tangential support. It would indicate that integration principle picks out special transitions (perhaps analogous to phase transitions in physical systems).
\item Effective Physical Constants: Our model could in principle output numerical values for some constants. We gave in earlier drafts some numerical coincidences: e.g. deriving the fine-structure constant $\alpha \approx 1/137$ from $\pi$ factors, or the proton-electron mass ratio $\approx 1836$ from combinatorial arguments. Frankly, those were more numerology than solid derivation. At this stage, we do not claim a robust derivation of constants; we can only say the framework is not in conflict with them. However, if in future we find a compelling calculation for one of these (say, showing that the only self-consistent network that yields chemistry has $\alpha = 1/137$), that would be a huge boon.
\item Gravitational Waves from Early Universe: A fun prediction is a primordial gravitational wave background peak around $10^{-8}$ Hz (near the range of pulsar timing arrays) - see 5.1.
\item Deviation from Unit Probability (Born rule) in extreme cases: Some emergent quantum theories find that the Born probability rule is not exact but approximate. Our framework, being well aligned with standard quantum formalism, likely preserves Born’s rule exactly (since Gleason’s theorem etc. would hold in the Hilbert space context). So we predict no violation of Born rule. If any experiment (like certain quantum gravity proposals or extended Leggett-Garg tests) saw a deviation, it might challenge the idea that standard quantum category accounts for everything.
\item Emergent Time Phenomena: If time is emergent, one might look for scenarios where what we think of as time could fluctuate or have an “entropy” of its own. Some quantum gravity theories talk about time becoming non-classical. Our model is more classical about time (it’s a well-defined order), so we wouldn’t expect observable fluctuations in the arrow of time or causal structure at human scales. If something like that were observed (say, signals of causal violations or variations in the flow of time not due to relativity), that would be hard to reconcile.
\end{itemize}

In essence, most near-term tests of our theory coincide with tests of other new physics: looking for dark matter as emergent vs particle, searching for signs of spacetime discreteness or deviations in gravitational behavior, and exploring the integration principle in complex systems. The theory’s strength is that it provides a coherent explanation for many puzzles at once (why these fields and particles, why these constants, why life arises). The risk is that it’s so broad it may be unfalsifiable if not made more specific. We acknowledge the need to sharpen it into a predictive framework.

\subsection{Primordial nano-Hertz gravitational-wave relic}
\label{subsec:nHz_GW}

\paragraph{Claim.}
The self-referential growth dynamics of \textsc{Deep-Thought} predict a
\emph{stochastic gravitational-wave background peaking near
\(f_{0}\!\sim\!10^{-8}\,\mathrm{Hz}\)}, i.e.\ in the nano-Hertz band now
probed by pulsar-timing arrays.

\paragraph{Derivation.}

\begin{enumerate}
  \item \textbf{Minimal coherent loop.}  
        After the first symmetry-breaking step the relational web
        contains a closed path of length
        \[
          \lambda_* \;=\; (d+n_g)\,\ell_{\mathrm P}
                     \;=\; 6\,\ell_{\mathrm P}
                     \;\simeq\;9.7\times10^{-35}\,\text{m},
        \]
        where \(d=3\), \(n_g=3\), and
        \(\ell_{\mathrm P}=1.616\times10^{-35}\,\text{m}\).
  \item \textbf{Fundamental tensor mode at emission.}  
        Treating the loop as a ring resonator,
        \[
          f_* \;=\;\frac{c}{\lambda_*}
               \;\approx\;\frac{3.00\times10^{8}}
                               {9.7\times10^{-35}}
               \;\simeq\;3.1\times10^{42}\,\text{Hz}.
        \]
  \item \textbf{Cosmological red-shift.}  
        Standard thermal history implies a total scale-factor growth  
        \(a_0/a_* \sim 10^{51}\)  
        (≈\,$10^{26}$ from inflation, \(10^{23}\) during the radiation
        era, \(10^{3.5}\) in the matter/\(\Lambda\) era), so
        \[
          f_0 \;=\; f_* \Bigl(\frac{a_*}{a_0}\Bigr)
               \;\approx\;3.1\times10^{42}\,\text{Hz}\times10^{-51}
               \;\simeq\;3\times10^{-9}\text{--}10^{-8}\,\text{Hz}.
        \]
\end{enumerate}

\paragraph{Observational outlook.}
A broad peak in this decade band would support the growth-layer scaling
in \textsc{Deep-Thought}; its absence (or a dominant peak far outside
\(10^{-10}\!-\!10^{-6}\,\mathrm{Hz}\)) would challenge the model.

\paragraph{Caveat.}
The \(10^{51}\) red-shift factor is based on a coarse-grained cosmology
and should ultimately be derived from the entropy budget of the
dagger-compact topos; the order-of-magnitude prediction remains
testable regardless of such refinements.

\section{Conclusion}

We have outlined a framework in which the Universe is a self-generated, self-consistent set of relationships, mathematically captured by a rich category that grows in complexity. This approach suggests that what we call the laws of physics are not transcendent prescriptions but rather patterns of consistency that emerge as the Universe “tries every possible relation” and only stable patterns persist. In this view, existence is its own explanation: the Universe exists because it can – it is the ultimate self-referential loop that bootstraps into ever greater complexity, producing observers within it who can ask why it exists in the first place. The power of this framework is its unifying capacity. We saw that by starting from a minimal assumption (an identity morphism on a unit object), we could qualitatively derive:

\begin{itemize}
\item The structural framework of quantum mechanics (objects $\mapsto$ state spaces, morphisms $\mapsto$ processes, tensor $\mapsto$ composite systems, dagger $\mapsto$ adjoint operations, etc.).
\item The dimensionality and basic symmetry structure of spacetime (3+1 D with Lorentz symmetry) as an outgrowth of how relations can consistently weave together\cite{tegmark1997}.
\item The existence of different particle types and forces, tied to the presence of certain symmetries and necessary interaction patterns (e.g., three generations for CP-violation\cite{kobayashi1973}, gauge bosons for local symmetries).
\item Higher-order phenomena like life and consciousness as continuation of the same relational principles at a new scale (high integration yields agency and awareness).
\end{itemize}

This broad scope is reminiscent of other “Theory of Everything” attempts (string theory, etc.), but our approach is distinct in that it does not introduce a whole new physical object (like strings) or assume a specific high-energy symmetry – instead, it leans on an even more abstract mathematical skeleton (category theory) and derives concrete structure from it. One may worry that this is too mathematical – are we implying “the universe is just math”? Perhaps in a sense we are, akin to Tegmark’s Mathematical Universe Hypothesis, but with the nuance that it’s relations (information) that are fundamental, not mathematical platonic forms devoid of context. This is aligned with John Wheeler’s aphorism, “It from bit”, where he suggests reality arises from yes-no questions (bits of information)\cite{wheeler1989}. We have essentially built a model of how that could be so, with category theory ensuring those bits of information knit together into the rich tapestry we see, rather than remaining random noise. Of course, much work remains. We need to formalize the “growth rule” for the category – possibly as a variational principle or a logical induction. We need to derive more quantitative results (right now, we qualitatively argued many points, but a full theory would let us compute things like the electron’s mass or the cosmological constant from first principles).

The cosmological constant problem, by the way, might find a natural resolution here: our vacuum is dynamic and self-referential, so maybe it cancels most vacuum energy except a tiny residue (that tiny curvature leftover is $\Lambda$). Indeed, one earlier formula we toyed with suppressed $\Lambda$ by many orders (tied to the size of the universe)\cite{carlip2017} – hinting that maybe the large number ($10^{122}$ discrepancy) could be explained by the universe’s age or total information content. In comparing to other approaches, we found kindred spirits: causal set theory values causality and discreteness\cite{bombelli1987}, Wolfram’s model values computational emergence\cite{wolfram2020}. Our approach synthesizes elements of those but within a single coherent algebraic structure that naturally includes logic and computation (via topos and category) and physics (via monoidal dagger compactness capturing quantum processes\cite{abramsky2009}). This suggests a deep connection between physical law and computation. The universe might essentially be performing a computation (perhaps similar to how a compiler resolves a self-referential definition) – and the output of that computation is the fabric of reality itself. If so, understanding physics becomes akin to understanding the theory of computation and category theory applied to nature. This could lead to new insights – for example, perhaps unsolved problems like unifying quantum mechanics with gravity become easier if seen as issues of functorial mappings between quantum categories and spacetime categories, something category theorists have started exploring. 

We conclude on a philosophical note: a self-referential universe might answer the age-old question “Why is there something rather than nothing?” with a bold reply: Because nothing is unstable; “something” is the only self-consistent option. The only way to have absolutely nothing is to have no questions posed. But the moment you conceive of “nothing”, you’ve posed a question (does nothing exist?), and to answer it, the universe must exist. In our model, $I$ (void) asked a question of itself via $\mathrm{id}_I$, and that seed of curiosity bloomed into everything. It’s a poetic vision, yet we’ve grounded it in mathematics enough to at least chase it scientifically. The road ahead will involve refining these ideas, connecting them more rigorously to known physics, and designing experiments or observations that could support or refute the framework. This work lies at a crossroads of theoretical physics, mathematics, and philosophy of science. By approaching it with the modern tools of category theory and computation, we hope to make progress on the most fundamental questions. In the words of American pragmatist philosopher Charles Sanders Peirce (who intriguingly worked on relational logic in the 19th century), “We only understand something when we understand the relations it has, and especially the habits (laws) it follows.” Reality, under this view, is nothing but an extremely intricate habit of self-relation, one that we have partially decoded as the laws of physics. Our task is to decode it fully – a challenge this relational-category framework invites us to undertake.

\bibliographystyle{alpha}
\bibliography{main}

\end{document}
